{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification with BERT\n",
    "\n",
    "Deep learning has been revolutionized by transformer models. Transformer based models like BERT are heavily used in NLP to solve tasks due to the rich numerical representations of text they provide. Here we will be discussing how to download a transformer model and then adapt it to solve a spam classification problem.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch13-Transormers-with-TF2-and-Huggingface/13.1_Spam_Classification_with_BERT.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.9.0 installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 10:57:26.889972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 10:57:26.911333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 10:57:26.912108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_models as tfm\n",
    "\n",
    "import time \n",
    "\n",
    "print(\"TensorFlow: {} installed\".format(tf.__version__))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the data\n",
    "\n",
    "For this, we will be using the spam classification dataset available [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip). It is a tab separated file with two columns. First column is a single word (ham/spam), where the second column contains the SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zip file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "# Downloading the data\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "        \n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'smsspamcollection.zip')):\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'smsspamcollection.zip'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The zip file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'SMSSpamCollection')):\n",
    "    with zipfile.ZipFile(os.path.join('data', 'smsspamcollection.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4827 ham and 747 spam\n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'Ok lar... Joking wif u oni...\\n', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\", 'U dun say so early hor... U c already then say...\\n', \"Nah I don't think he goes to usf, he lives around here though\\n\"]\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Code listing 13.1\n",
    "import numpy as np\n",
    "\n",
    "# Inputs and labels will be stored in this\n",
    "inputs = []\n",
    "labels = []\n",
    "# Total number of instances for spam and ham\n",
    "n_ham, n_spam = 0,0\n",
    "with open(os.path.join('data', 'SMSSpamCollection'), 'r') as f:\n",
    "    for r in f:        \n",
    "        # Ham input\n",
    "        if r.startswith('ham'):\n",
    "            label = 0\n",
    "            txt = r[4:]\n",
    "            n_ham += 1\n",
    "        # Spam input\n",
    "        elif r.startswith('spam'):\n",
    "            label = 1\n",
    "            txt = r[5:]\n",
    "            n_spam += 1\n",
    "        inputs.append(txt)\n",
    "        labels.append(label)\n",
    "        \n",
    "print(\"Found {} ham and {} spam\".format(n_ham, n_spam))\n",
    "print(inputs[:5])\n",
    "\n",
    "# Convert them to arrays\n",
    "inputs = np.array(inputs).reshape(-1,1)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting train/valid/test\n",
    "\n",
    "Here we will split the data to three sets using `imbalanced-learn` library. Specifically we,\n",
    "\n",
    "* Create a balanced test set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced validation set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced train set from the left over instances (undersampled using Near miss algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistics\n",
      "0    100\n",
      "1    100\n",
      "dtype: int64\n",
      "Valid statistics\n",
      "0    100\n",
      "1    100\n",
      "dtype: int64\n",
      "Train statistics\n",
      "0    4627\n",
      "1     547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "from imblearn.under_sampling import  NearMiss, RandomUnderSampler\n",
    "\n",
    "\n",
    "n=100 # Number of instances for each class for train/validation sets\n",
    "rus = RandomUnderSampler(sampling_strategy={0:n, 1:n}, random_state=random_seed)\n",
    "rus.fit_resample(inputs, labels)\n",
    "\n",
    "# Get test indices\n",
    "test_inds = rus.sample_indices_\n",
    "test_x, test_y = inputs[test_inds], np.array(labels)[test_inds]\n",
    "print(\"Test statistics\")\n",
    "print(pd.Series(test_y).value_counts())\n",
    "\n",
    "# Get rest (train + valid)\n",
    "rest_inds = [i for i in range(inputs.shape[0]) if i not in test_inds]\n",
    "rest_x, rest_y = inputs[rest_inds], labels[rest_inds]\n",
    "\n",
    "# Get valid indices\n",
    "rus.fit_resample(rest_x, rest_y)\n",
    "valid_inds = rus.sample_indices_\n",
    "valid_x, valid_y = rest_x[valid_inds], rest_y[valid_inds]\n",
    "print(\"Valid statistics\")\n",
    "print(pd.Series(valid_y).value_counts())\n",
    "\n",
    "# Rest goes in training\n",
    "train_inds = [i for i in range(rest_x.shape[0]) if i not in valid_inds]\n",
    "train_x, train_y = rest_x[train_inds], rest_y[train_inds]\n",
    "print(\"Train statistics\")\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    547\n",
      "1    547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To use near miss algorithm, we need a numerical representation of the messages\n",
    "# We will use the bag of words representation\n",
    "countvec = CountVectorizer()\n",
    "train_bow = countvec.fit_transform(train_x.reshape(-1).tolist())\n",
    "\n",
    "# NearMiss is a common undersampling technique\n",
    "oss = NearMiss()\n",
    "X_res, y_res = oss.fit_resample(train_bow, train_y)\n",
    "train_inds = oss.sample_indices_\n",
    "\n",
    "train_x, train_y = train_x[train_inds], train_y[train_inds]\n",
    "\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the BERT model\n",
    "\n",
    "Here we download the BERT model from the TensorFlow hub. and create a Keras layer from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 10:57:31.691399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-27 10:57:31.692025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 10:57:31.692392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 10:57:31.692702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 10:57:32.085915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 10:57:32.086256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 10:57:32.086547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 10:57:32.086812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6343 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "bert_layer = hub.KerasLayer(bert_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inputs for the BERT model\n",
    "\n",
    "BERT model needs three inputs,\n",
    "\n",
    "* Input word IDs - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* Input mask - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* Segment IDs - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 768)\n",
      "(None, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "\n",
    "# Contains input token ids\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "# Contains input mask values\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "# Contains input type (whether token belongs to sequence A or B) values\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True)\n",
    "\n",
    "# Check the outputs of the Bert layer\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "print(pooled_output.shape)\n",
    "print(sequence_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file is at: b'/tmp/tfhub_modules/ce53fe6769d2ac3a260e92555120c54e1aecbea6/assets/vocab.txt'\n",
      "Configuration do_lower_case of BERT: True\n"
     ]
    }
   ],
   "source": [
    "# Get the vocab file path from the BERT layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "print(\"Vocabulary file is at: {}\".format(vocab_file))\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "print(\"Configuration {} of BERT: {}\".format(\"do_lower_case\", do_lower_case))\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = tfm.nlp.layers.FastWordpieceBertTokenizer(vocab_file=vocab_file, lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding tokenization in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['She sells seashells by the seashore']\n",
      "Tokens IDs generated by BERT: [ 2016 15187 11915 18223  2015  2011  1996 11915 16892]\n",
      "Tokens generated by BERT: ['she', 'sells', 'seas', '##hell', '##s', 'by', 'the', 'seas', '##hore']\n"
     ]
    }
   ],
   "source": [
    "text = [\"She sells seashells by the seashore\"]\n",
    "print(\"Original text: {}\".format(text))\n",
    "#tokens = tokenizer.tokenize(text)\n",
    "tokens = tf.reshape(tokenizer(text), [-1])\n",
    "print(\"Tokens IDs generated by BERT: {}\".format(tokens))\n",
    "ids = [tokenizer._vocab[tid] for tid in tokens] \n",
    "#tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens generated by BERT: {}\".format(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens used by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS] has ID: 101\n",
      "Token: [SEP] has ID: 102\n",
      "Token: [MASK] has ID: 103\n",
      "Token: [PAD] has ID: 0\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]', '[MASK]', '[PAD]']\n",
    "ids = [tokenizer._vocab.index(tok) for tok in special_tokens]\n",
    "for t, i in zip(special_tokens, ids):\n",
    "    print(\"Token: {} has ID: {}\".format(t, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean       35.924132\n",
       "std        19.560793\n",
       "min        12.000000\n",
       "25%        18.000000\n",
       "50%        25.000000\n",
       "75%        55.000000\n",
       "90%        62.000000\n",
       "max        77.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13.2\n",
    "# Code listing 13.2\n",
    "def encode_sentence(s):\n",
    "    \"\"\" Encode a given sentence by tokenizing it and adding special tokens \"\"\"\n",
    "    \n",
    "    tokens = list(tf.reshape(tokenizer([\"CLS\" + s + \"[SEP]\"]), [-1]))\n",
    "    return tokens\n",
    "\n",
    "seq_lengths = pd.Series([len(encode_sentence(str(s))) for s in train_x])\n",
    "seq_lengths.describe(percentiles=[0.25, 0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the correct input format for BERT\n",
    "\n",
    "BERT model needs three inputs. These are formed into a dictionary having the following keys.\n",
    "\n",
    "* `input_word_ids` - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* `input_mask` - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* `input_type_ids` - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)\n",
    "\n",
    "`tf-models` library provides a convenient class to pack a given sentence (or a list of sentences) to this format called `BertPackInputs`. We'll use this to create our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_word_ids': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[  101,  2016, 15187, 11915, 18223,  2015,  2011,  1996, 11915,\n",
      "        16892,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'input_type_ids': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 60\n",
    "\n",
    "packer = tfm.nlp.layers.BertPackInputs(\n",
    "    seq_length=max_seq_length,\n",
    "    special_tokens_dict = tokenizer.get_special_tokens_dict()\n",
    ")\n",
    "\n",
    "text = [\"She sells seashells by the seashore\"]\n",
    "\n",
    "tok1 = tokenizer(text)\n",
    "\n",
    "packed = packer(tok1)\n",
    "\n",
    "print(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1094 training samples\n",
      "Added 200 validation samples\n",
      "Added 200 test samples\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Code listing 13.3\n",
    "def get_bert_inputs(tokenizer, docs,max_seq_len=None):\n",
    "    \"\"\" Generate inputs for BERT using a set of documents \"\"\"\n",
    "    \n",
    "    packer = tfm.nlp.layers.BertPackInputs(\n",
    "        seq_length=max_seq_length,\n",
    "        special_tokens_dict = tokenizer.get_special_tokens_dict()\n",
    "    )\n",
    "    \n",
    "    packed = packer(tokenizer(docs))\n",
    "    \n",
    "    packed_numpy = dict([(k, v.numpy()) for k,v in packed.items()])\n",
    "    # Final output\n",
    "    return packed_numpy\n",
    "\n",
    "# Creating train/validation/test data\n",
    "train_inputs = get_bert_inputs(tokenizer, train_x, max_seq_len=60)\n",
    "print(f\"Added {len(train_inputs['input_word_ids'])} training samples\")\n",
    "\n",
    "valid_inputs = get_bert_inputs(tokenizer, valid_x, max_seq_len=60)\n",
    "print(f\"Added {len(valid_inputs['input_word_ids'])} validation samples\")\n",
    "\n",
    "test_inputs = get_bert_inputs(tokenizer, test_x, max_seq_len=60)\n",
    "print(f\"Added {len(test_inputs['input_word_ids'])} test samples\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "random_seed = 4321\n",
    "np.random.seed(4321)\n",
    "\n",
    "# Shuffling training data as a precaution\n",
    "train_inds = np.random.permutation(len(train_inputs[\"input_word_ids\"]))\n",
    "train_inputs = dict([(k, v[train_inds]) for k, v in train_inputs.items()])\n",
    "train_y = train_y[train_inds]\n",
    "\n",
    "print(train_y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a downstream classifier from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Config\n",
      "{'attention_dropout_rate': 0.1, 'dropout_rate': 0.1, 'hidden_activation': 'gelu', 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_layers': 12, 'type_vocab_size': 2, 'vocab_size': 30522}\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "import yaml\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/master/official/nlp/configs/models/bert_en_uncased_base.yaml\n",
    "with open(os.path.join(\"data\", \"bert_en_uncased_base.yaml\"), 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)['task']['model']['encoder']['bert']\n",
    "\n",
    "\n",
    "# Print BERT config\n",
    "print(\"BERT Config\")\n",
    "print(config_dict)\n",
    "\n",
    "encoder_config = tfm.nlp.encoders.EncoderConfig({\n",
    "    'type':'bert',\n",
    "    'bert': config_dict\n",
    "})\n",
    "\n",
    "\n",
    "bert_encoder = tfm.nlp.encoders.build_encoder(encoder_config)\n",
    "\n",
    "\n",
    "# Generating a classifier and the encoder\n",
    "bert_classifier = tfm.nlp.models.BertClassifier(network=bert_encoder, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code listing 13.4\n",
    "# Set up epochs and steps\n",
    "epochs = 3\n",
    "batch_size = 52\n",
    "eval_batch_size = 52\n",
    "\n",
    "train_data_size = train_x.shape[0]\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(num_train_steps * 0.1)\n",
    "\n",
    "initial_learning_rate = 2e-6\n",
    "\n",
    "# Define the decay\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    end_learning_rate=0,\n",
    "    decay_steps=num_train_steps)\n",
    "\n",
    "# Define learning rate schedule\n",
    "warmup_schedule = tfm.optimization.lr_schedule.LinearWarmup(\n",
    "    warmup_learning_rate = 1e-10,\n",
    "    after_warmup_lr_sched = linear_decay,\n",
    "    warmup_steps = warmup_steps\n",
    ")\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = tf.keras.optimizers.experimental.Adam(\n",
    "    learning_rate = warmup_schedule\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BERT and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 10:58:26.226962: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x55fa006c5c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-07-27 10:58:26.226982: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
      "2022-07-27 10:58:26.230548: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-07-27 10:58:26.287380: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-07-27 10:58:26.335423: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-07-27 10:58:26.389197: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-07-27 10:58:26.498518: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-07-27 10:58:26.606398: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-07-27 10:58:26.872563: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-07-27 10:58:26.974669: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-07-27 10:58:37.231340: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-07-27 10:58:37.391005: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 34s 438ms/step - loss: 0.5932 - accuracy: 0.6901 - val_loss: 0.5333 - val_accuracy: 0.7800\n",
      "Epoch 2/3\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.3812 - accuracy: 0.9196 - val_loss: 0.5021 - val_accuracy: 0.7450\n",
      "Epoch 3/3\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 0.3050 - accuracy: 0.9552 - val_loss: 0.4841 - val_accuracy: 0.7600\n",
      "It took 54.34916830062866 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model\n",
    "bert_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Train the model\n",
    "bert_classifier.fit(\n",
    "      x=train_inputs, \n",
    "      y=train_y,\n",
    "      validation_data=(valid_inputs, valid_y),\n",
    "      validation_batch_size=eval_batch_size,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      epochs=epochs\n",
    ")\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:HDF5 format does not save weights of `optimizer_experimental.Optimizer`, your optimizer will be recompiled at loading time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:HDF5 format does not save weights of `optimizer_experimental.Optimizer`, your optimizer will be recompiled at loading time.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(bert_classifier, os.path.join('models', 'bert_spam.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 87ms/step - loss: 0.5118 - accuracy: 0.7450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5118228197097778, 0.7450000047683716]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13.2\n",
    "bert_classifier.evaluate(test_inputs, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
