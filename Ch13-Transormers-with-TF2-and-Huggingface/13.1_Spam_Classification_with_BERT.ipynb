{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification with BERT\n",
    "\n",
    "Deep learning has been revolutionized by transformer models. Transformer based models like BERT are heavily used in NLP to solve tasks due to the rich numerical representations of text they provide. Here we will be discussing how to download a transformer model and then adapt it to solve a spam classification problem.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch13-Transormers-with-TF2-and-Huggingface/13.1_Spam_Classification_with_BERT.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.9.0 installed\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_models as tfm\n",
    "#from official import nlp\n",
    "#from official.nlp import bert\n",
    "#import official.nlp.optimization\n",
    "#import official.nlp.bert.bert_models\n",
    "#import official.nlp.bert.configs\n",
    "import time \n",
    "\n",
    "print(\"TensorFlow: {} installed\".format(tf.__version__))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the data\n",
    "\n",
    "For this, we will be using the spam classification dataset available [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip). It is a tab separated file with two columns. First column is a single word (ham/spam), where the second column contains the SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zip file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "# Downloading the data\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "        \n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'smsspamcollection.zip')):\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'smsspamcollection.zip'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The zip file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'SMSSpamCollection')):\n",
    "    with zipfile.ZipFile(os.path.join('data', 'smsspamcollection.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4827 ham and 747 spam\n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'Ok lar... Joking wif u oni...\\n', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\", 'U dun say so early hor... U c already then say...\\n', \"Nah I don't think he goes to usf, he lives around here though\\n\"]\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Code listing 13.1\n",
    "import numpy as np\n",
    "\n",
    "# Inputs and labels will be stored in this\n",
    "inputs = []\n",
    "labels = []\n",
    "# Total number of instances for spam and ham\n",
    "n_ham, n_spam = 0,0\n",
    "with open(os.path.join('data', 'SMSSpamCollection'), 'r') as f:\n",
    "    for r in f:        \n",
    "        # Ham input\n",
    "        if r.startswith('ham'):\n",
    "            label = 0\n",
    "            txt = r[4:]\n",
    "            n_ham += 1\n",
    "        # Spam input\n",
    "        elif r.startswith('spam'):\n",
    "            label = 1\n",
    "            txt = r[5:]\n",
    "            n_spam += 1\n",
    "        inputs.append(txt)\n",
    "        labels.append(label)\n",
    "        \n",
    "print(\"Found {} ham and {} spam\".format(n_ham, n_spam))\n",
    "print(inputs[:5])\n",
    "\n",
    "# Convert them to arrays\n",
    "inputs = np.array(inputs).reshape(-1,1)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting train/valid/test\n",
    "\n",
    "Here we will split the data to three sets using `imbalanced-learn` library. Specifically we,\n",
    "\n",
    "* Create a balanced test set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced validation set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced train set from the left over instances (undersampled using Near miss algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistics\n",
      "0    100\n",
      "1    100\n",
      "dtype: int64\n",
      "Valid statistics\n",
      "0    100\n",
      "1    100\n",
      "dtype: int64\n",
      "Train statistics\n",
      "0    4627\n",
      "1     547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "from imblearn.under_sampling import  NearMiss, RandomUnderSampler\n",
    "\n",
    "\n",
    "n=100 # Number of instances for each class for train/validation sets\n",
    "rus = RandomUnderSampler(sampling_strategy={0:n, 1:n}, random_state=random_seed)\n",
    "rus.fit_resample(inputs, labels)\n",
    "\n",
    "# Get test indices\n",
    "test_inds = rus.sample_indices_\n",
    "test_x, test_y = inputs[test_inds], np.array(labels)[test_inds]\n",
    "print(\"Test statistics\")\n",
    "print(pd.Series(test_y).value_counts())\n",
    "\n",
    "# Get rest (train + valid)\n",
    "rest_inds = [i for i in range(inputs.shape[0]) if i not in test_inds]\n",
    "rest_x, rest_y = inputs[rest_inds], labels[rest_inds]\n",
    "\n",
    "# Get valid indices\n",
    "rus.fit_resample(rest_x, rest_y)\n",
    "valid_inds = rus.sample_indices_\n",
    "valid_x, valid_y = rest_x[valid_inds], rest_y[valid_inds]\n",
    "print(\"Valid statistics\")\n",
    "print(pd.Series(valid_y).value_counts())\n",
    "\n",
    "# Rest goes in training\n",
    "train_inds = [i for i in range(rest_x.shape[0]) if i not in valid_inds]\n",
    "train_x, train_y = rest_x[train_inds], rest_y[train_inds]\n",
    "print(\"Train statistics\")\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    547\n",
      "1    547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To use near miss algorithm, we need a numerical representation of the messages\n",
    "# We will use the bag of words representation\n",
    "countvec = CountVectorizer()\n",
    "train_bow = countvec.fit_transform(train_x.reshape(-1).tolist())\n",
    "\n",
    "# NearMiss is a common undersampling technique\n",
    "oss = NearMiss()\n",
    "X_res, y_res = oss.fit_resample(train_bow, train_y)\n",
    "train_inds = oss.sample_indices_\n",
    "\n",
    "train_x, train_y = train_x[train_inds], train_y[train_inds]\n",
    "\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the BERT model\n",
    "\n",
    "Here we download the BERT model from the TensorFlow hub. and create a Keras layer from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "bert_layer = hub.KerasLayer(bert_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inputs for the BERT model\n",
    "\n",
    "BERT model needs three inputs,\n",
    "\n",
    "* Input word IDs - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* Input mask - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* Segment IDs - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 768)\n",
      "(None, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "\n",
    "# Contains input token ids\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "# Contains input mask values\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "# Contains input type (whether token belongs to sequence A or B) values\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True)\n",
    "\n",
    "# Check the outputs of the Bert layer\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "print(pooled_output.shape)\n",
    "print(sequence_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file is at: b'C:\\\\Users\\\\thush\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\ce53fe6769d2ac3a260e92555120c54e1aecbea6\\\\assets\\\\vocab.txt'\n",
      "Configuration do_lower_case of BERT: True\n"
     ]
    }
   ],
   "source": [
    "# Get the vocab file path from the BERT layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "print(\"Vocabulary file is at: {}\".format(vocab_file))\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "print(\"Configuration {} of BERT: {}\".format(\"do_lower_case\", do_lower_case))\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = tfm.nlp.layers.FastWordpieceBertTokenizer(vocab_file=vocab_file, lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding tokenization in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['She sells seashells by the seashore']\n",
      "Tokens IDs generated by BERT: [ 2016 15187 11915 18223  2015  2011  1996 11915 16892]\n",
      "Tokens generated by BERT: ['she', 'sells', 'seas', '##hell', '##s', 'by', 'the', 'seas', '##hore']\n"
     ]
    }
   ],
   "source": [
    "text = [\"She sells seashells by the seashore\"]\n",
    "print(\"Original text: {}\".format(text))\n",
    "#tokens = tokenizer.tokenize(text)\n",
    "tokens = tf.reshape(tokenizer(text), [-1])\n",
    "print(\"Tokens IDs generated by BERT: {}\".format(tokens))\n",
    "ids = [tokenizer._vocab[tid] for tid in tokens] \n",
    "#tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens generated by BERT: {}\".format(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens used by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS] has ID: 101\n",
      "Token: [SEP] has ID: 102\n",
      "Token: [MASK] has ID: 103\n",
      "Token: [PAD] has ID: 0\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]', '[MASK]', '[PAD]']\n",
    "ids = [tokenizer._vocab.index(tok) for tok in special_tokens]\n",
    "for t, i in zip(special_tokens, ids):\n",
    "    print(\"Token: {} has ID: {}\".format(t, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the sentence in a way suitable to BERT\n",
    "\n",
    "Here we will add the `[CLS]` token to the front of the input and `[SEP]` to the back of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13.2\n",
    "# Code listing 13.2\n",
    "def encode_sentence(s):\n",
    "    \"\"\" Encode a given sentence by tokenizing it and adding special tokens \"\"\"\n",
    "    \n",
    "    tokens = list(tf.reshape(tokenizer([\"CLS\" + s + \"[SEP]\"]), [-1]))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean       36.256856\n",
       "std        19.915669\n",
       "min        12.000000\n",
       "25%        18.000000\n",
       "50%        25.000000\n",
       "75%        56.000000\n",
       "90%        63.000000\n",
       "max        79.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = pd.Series([len(encode_sentence(str(s))) for s in train_x])\n",
    "seq_lengths.describe(percentiles=[0.25, 0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the correct input format for BERT\n",
    "\n",
    "BERT model needs three inputs. These are formed into a dictionary having the following keys.\n",
    "\n",
    "* `input_word_ids` - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* `input_mask` - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* `input_type_ids` - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_word_ids': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[  101,  2016, 15187, 11915, 18223,  2015,  2011,  1996, 11915,\n",
      "        16892,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]])>, 'input_mask': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'input_type_ids': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 60\n",
    "\n",
    "packer = tfm.nlp.layers.BertPackInputs(\n",
    "    seq_length=max_seq_length,\n",
    "    special_tokens_dict = tokenizer.get_special_tokens_dict()\n",
    ")\n",
    "\n",
    "text = [\"She sells seashells by the seashore\"]\n",
    "\n",
    "tok1 = tokenizer(text)\n",
    "\n",
    "packed = packer(tok1)\n",
    "\n",
    "print(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1094 training samples\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Code listing 13.3\n",
    "def get_bert_inputs(tokenizer, docs,max_seq_len=None):\n",
    "    \"\"\" Generate inputs for BERT using a set of documents \"\"\"\n",
    "    \n",
    "    packer = tfm.nlp.layers.BertPackInputs(\n",
    "        seq_length=max_seq_length,\n",
    "        special_tokens_dict = tokenizer.get_special_tokens_dict()\n",
    "    )\n",
    "    \n",
    "    packed = packer(tokenizer(docs))\n",
    "    # Final output\n",
    "    return packed\n",
    "\n",
    "# Creating train/validation/test data\n",
    "train_inputs = get_bert_inputs(tokenizer, train_x, max_seq_len=60)\n",
    "print(f\"Added {len(train_inputs['input_word_ids'])} training samples\")\n",
    "\n",
    "valid_inputs = get_bert_inputs(tokenizer, valid_x, max_seq_len=60)\n",
    "test_inputs = get_bert_inputs(tokenizer, test_x, max_seq_len=60)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a downstream classifier from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Config\n",
      "{'attention_dropout_rate': 0.1, 'dropout_rate': 0.1, 'hidden_activation': 'gelu', 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_layers': 12, 'type_vocab_size': 2, 'vocab_size': 30522}\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "import yaml\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/master/official/nlp/configs/models/bert_en_uncased_base.yaml\n",
    "with open(os.path.join(\"data\", \"bert_en_uncased_base.yaml\"), 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)['task']['model']['encoder']['bert']\n",
    "\n",
    "\n",
    "# Print BERT config\n",
    "print(\"BERT Config\")\n",
    "print(config_dict)\n",
    "\n",
    "encoder_config = tfm.nlp.encoders.EncoderConfig({\n",
    "    'type':'bert',\n",
    "    'bert': config_dict\n",
    "})\n",
    "\n",
    "\n",
    "bert_encoder = tfm.nlp.encoders.build_encoder(encoder_config)\n",
    "\n",
    "\n",
    "# Generating a classifier and the encoder\n",
    "bert_classifier = tfm.nlp.models.BertClassifier(network=bert_encoder, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code listing 13.4\n",
    "# Set up epochs and steps\n",
    "epochs = 3\n",
    "batch_size = 1\n",
    "eval_batch_size = 2\n",
    "\n",
    "train_data_size = train_x.shape[0]\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "initial_learning_rate = 2e-6\n",
    "\n",
    "# Define the decay\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    end_learning_rate=0,\n",
    "    decay_steps=num_train_steps)\n",
    "\n",
    "# Define learning rate schedule\n",
    "warmup_schedule = tfm.optimization.lr_schedule.LinearWarmup(\n",
    "    warmup_learning_rate = 0,\n",
    "    after_warmup_lr_sched = linear_decay,\n",
    "    warmup_steps = warmup_steps\n",
    ")\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = tf.keras.optimizers.experimental.Adam(\n",
    "    learning_rate = warmup_schedule\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BERT and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py\", line 893, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 401, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 811, in apply_gradients\n        super().apply_gradients(grads_and_vars)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 425, in apply_gradients\n        self.build(trainable_variables)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\adam.py\", line 131, in build\n        self.add_variable_from_reference(\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 767, in add_variable_from_reference\n        return super().add_variable_from_reference(model_variable, variable_name,\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 372, in add_variable_from_reference\n        initial_value = tf.zeros(\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[768,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mbert_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t2\u001b[38;5;241m-\u001b[39mt1))\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1127\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1128\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: in user code:\n\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py\", line 893, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 401, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 811, in apply_gradients\n        super().apply_gradients(grads_and_vars)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 425, in apply_gradients\n        self.build(trainable_variables)\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\adam.py\", line 131, in build\n        self.add_variable_from_reference(\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 767, in add_variable_from_reference\n        return super().add_variable_from_reference(model_variable, variable_name,\n    File \"C:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 372, in add_variable_from_reference\n        initial_value = tf.zeros(\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[768,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model\n",
    "bert_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Train the model\n",
    "bert_classifier.fit(\n",
    "      x=train_inputs, \n",
    "      y=train_y,\n",
    "      validation_data=(valid_inputs, valid_y),\n",
    "      validation_batch_size=eval_batch_size,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(hub_classifier, os.path.join('models', 'bert_spam.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 7s 930ms/step - loss: 0.5349 - accuracy: 0.8300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5349494218826294, 0.8299999833106995]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13.2\n",
    "hub_classifier.evaluate(test_inputs, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
