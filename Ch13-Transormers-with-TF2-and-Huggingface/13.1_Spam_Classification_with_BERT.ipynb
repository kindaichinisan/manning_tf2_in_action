{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification with BERT\n",
    "\n",
    "Deep learning has been revolutionized by transformer models. Transformer based models like BERT are heavily used in NLP to solve tasks due to the rich numerical representations of text they provide. Here we will be discussing how to download a transformer model and then adapt it to solve a spam classification problem.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch13-Transormers-with-TF2-and-Huggingface/13.1_Spam_Classification_with_BERT.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.9.0 installed\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_models as tfm\n",
    "\n",
    "import time \n",
    "\n",
    "print(\"TensorFlow: {} installed\".format(tf.__version__))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the data\n",
    "\n",
    "For this, we will be using the spam classification dataset available [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip). It is a tab separated file with two columns. First column is a single word (ham/spam), where the second column contains the SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zip file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "# Downloading the data\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "        \n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'smsspamcollection.zip')):\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'smsspamcollection.zip'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The zip file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'SMSSpamCollection')):\n",
    "    with zipfile.ZipFile(os.path.join('data', 'smsspamcollection.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4827 ham and 747 spam\n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'Ok lar... Joking wif u oni...\\n', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\", 'U dun say so early hor... U c already then say...\\n', \"Nah I don't think he goes to usf, he lives around here though\\n\"]\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Code listing 13.1\n",
    "import numpy as np\n",
    "\n",
    "# Inputs and labels will be stored in this\n",
    "inputs = []\n",
    "labels = []\n",
    "# Total number of instances for spam and ham\n",
    "n_ham, n_spam = 0,0\n",
    "with open(os.path.join('data', 'SMSSpamCollection'), 'r') as f:\n",
    "    for r in f:        \n",
    "        # Ham input\n",
    "        if r.startswith('ham'):\n",
    "            label = 0\n",
    "            txt = r[4:]\n",
    "            n_ham += 1\n",
    "        # Spam input\n",
    "        elif r.startswith('spam'):\n",
    "            label = 1\n",
    "            txt = r[5:]\n",
    "            n_spam += 1\n",
    "        inputs.append(txt)\n",
    "        labels.append(label)\n",
    "        \n",
    "print(\"Found {} ham and {} spam\".format(n_ham, n_spam))\n",
    "print(inputs[:5])\n",
    "\n",
    "# Convert them to arrays\n",
    "inputs = np.array(inputs).reshape(-1,1)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting train/valid/test\n",
    "\n",
    "Here we will split the data to three sets using `imbalanced-learn` library. Specifically we,\n",
    "\n",
    "* Create a balanced test set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced validation set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced train set from the left over instances (undersampled using Near miss algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistics\n",
      "0    100\n",
      "1    100\n",
      "dtype: int64\n",
      "Valid statistics\n",
      "0    100\n",
      "1    100\n",
      "dtype: int64\n",
      "Train statistics\n",
      "0    4627\n",
      "1     547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "from imblearn.under_sampling import  NearMiss, RandomUnderSampler\n",
    "\n",
    "\n",
    "n=100 # Number of instances for each class for train/validation sets\n",
    "rus = RandomUnderSampler(sampling_strategy={0:n, 1:n}, random_state=random_seed)\n",
    "rus.fit_resample(inputs, labels)\n",
    "\n",
    "# Get test indices\n",
    "test_inds = rus.sample_indices_\n",
    "test_x, test_y = inputs[test_inds], np.array(labels)[test_inds]\n",
    "print(\"Test statistics\")\n",
    "print(pd.Series(test_y).value_counts())\n",
    "\n",
    "# Get rest (train + valid)\n",
    "rest_inds = [i for i in range(inputs.shape[0]) if i not in test_inds]\n",
    "rest_x, rest_y = inputs[rest_inds], labels[rest_inds]\n",
    "\n",
    "# Get valid indices\n",
    "rus.fit_resample(rest_x, rest_y)\n",
    "valid_inds = rus.sample_indices_\n",
    "valid_x, valid_y = rest_x[valid_inds], rest_y[valid_inds]\n",
    "print(\"Valid statistics\")\n",
    "print(pd.Series(valid_y).value_counts())\n",
    "\n",
    "# Rest goes in training\n",
    "train_inds = [i for i in range(rest_x.shape[0]) if i not in valid_inds]\n",
    "train_x, train_y = rest_x[train_inds], rest_y[train_inds]\n",
    "print(\"Train statistics\")\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    547\n",
      "1    547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To use near miss algorithm, we need a numerical representation of the messages\n",
    "# We will use the bag of words representation\n",
    "countvec = CountVectorizer()\n",
    "train_bow = countvec.fit_transform(train_x.reshape(-1).tolist())\n",
    "\n",
    "# NearMiss is a common undersampling technique\n",
    "oss = NearMiss()\n",
    "X_res, y_res = oss.fit_resample(train_bow, train_y)\n",
    "train_inds = oss.sample_indices_\n",
    "\n",
    "train_x, train_y = train_x[train_inds], train_y[train_inds]\n",
    "\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the BERT model\n",
    "\n",
    "Here we download the BERT model from the TensorFlow hub. and create a Keras layer from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "bert_layer = hub.KerasLayer(bert_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inputs for the BERT model\n",
    "\n",
    "BERT model needs three inputs,\n",
    "\n",
    "* Input word IDs - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* Input mask - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* Segment IDs - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Contains input type (whether token belongs to sequence A or B) values\u001b[39;00m\n\u001b[0;32m     10\u001b[0m input_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(max_seq_length,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32,\n\u001b[0;32m     11\u001b[0m                                     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m bert_layer \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m pooled_output, sequence_output \u001b[38;5;241m=\u001b[39m bert_layer([input_word_ids, input_mask, input_type_ids])\n\u001b[0;32m     17\u001b[0m hub_encoder \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(\n\u001b[0;32m     18\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_word_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_word_ids, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_mask, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_type_ids}, \n\u001b[0;32m     19\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m[pooled_output, sequence_output]\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:153\u001b[0m, in \u001b[0;36mKerasLayer.__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m data_structures\u001b[38;5;241m.\u001b[39mNoDependency(\n\u001b[0;32m    150\u001b[0m       _convert_nest_to_shapes(output_shape))\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_options \u001b[38;5;241m=\u001b[39m load_options\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func \u001b[38;5;241m=\u001b[39m \u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_training_argument \u001b[38;5;241m=\u001b[39m func_has_training_argument(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hub_module_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:449\u001b[0m, in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n\u001b[0;32m    447\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# Expected before TF2.4.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     set_load_options \u001b[38;5;241m=\u001b[39m load_options\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_load_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow_hub\\module_v2.py:106\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    103\u001b[0m   obj \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload_v2(\n\u001b[0;32m    104\u001b[0m       module_path, tags\u001b[38;5;241m=\u001b[39mtags, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m   obj \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m obj\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m is_hub_module_v1  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:782\u001b[0m, in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m    781\u001b[0m   export_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(export_dir)\n\u001b[1;32m--> 782\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:912\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[0;32m    911\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 912\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_graph_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_model_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mckpt_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m         \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:189\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_all()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_options\u001b[38;5;241m.\u001b[39mexperimental_skip_checkpoint:\n\u001b[1;32m--> 189\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes:\n\u001b[0;32m    191\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, tracking\u001b[38;5;241m.\u001b[39mCapturableResource):\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:507\u001b[0m, in \u001b[0;36mLoader._restore_checkpoint\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m   load_status\u001b[38;5;241m.\u001b[39massert_nontrivial_match()\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 507\u001b[0m   load_status \u001b[38;5;241m=\u001b[39m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m   load_status\u001b[38;5;241m.\u001b[39massert_existing_objects_matched()\n\u001b[0;32m    509\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m load_status\u001b[38;5;241m.\u001b[39m_checkpoint\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1469\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1459\u001b[0m object_graph_proto\u001b[38;5;241m.\u001b[39mParseFromString(object_graph_string)\n\u001b[0;32m   1460\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _CheckpointRestoreCoordinator(\n\u001b[0;32m   1461\u001b[0m     object_graph_proto\u001b[38;5;241m=\u001b[39mobject_graph_proto,\n\u001b[0;32m   1462\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     graph_view\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view,\n\u001b[0;32m   1467\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m   1468\u001b[0m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpointPosition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m-> 1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;66;03m# Attached dependencies are not attached to the root, so should be restored\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;66;03m# separately.\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view\u001b[38;5;241m.\u001b[39mattached_dependencies:\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:295\u001b[0m, in \u001b[0;36mCheckpointPosition.restore\u001b[1;34m(self, trackable)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[0;32m    292\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_object(trackable):\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# This object's correspondence with a checkpointed object is new, so\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# process deferred restorations for it and its dependencies.\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m     restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mtrackable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_from_checkpoint_position\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m restore_ops:\n\u001b[0;32m    297\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint\u001b[38;5;241m.\u001b[39mnew_restore_ops(restore_ops)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:1060\u001b[0m, in \u001b[0;36mTrackable._restore_from_checkpoint_position\u001b[1;34m(self, checkpoint_position)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   _queue_children_for_restoration(current_position, visit_queue)\n\u001b[0;32m   1057\u001b[0m   checkpoint_util\u001b[38;5;241m.\u001b[39mqueue_slot_variables(current_position, visit_queue)\n\u001b[0;32m   1059\u001b[0m restore_ops\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m-> 1060\u001b[0m     \u001b[43mcurrent_position\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_saveables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_saveables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mpython_saveables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mregistered_savers\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:351\u001b[0m, in \u001b[0;36m_CheckpointRestoreCoordinator.restore_saveables\u001b[1;34m(self, tensor_saveables, python_saveables, registered_savers)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tensor_saveables\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m validated_names:\n\u001b[0;32m    346\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    347\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaveable keys changed when validating. Got back \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_saveables\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, was expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidated_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    349\u001b[0m new_restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDeviceSaver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidated_saveables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistered_savers\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_path_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    353\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m name, restore_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(new_restore_ops\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:415\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    413\u001b[0m   restore_ops \u001b[38;5;241m=\u001b[39m tf_function_restore()\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m   restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_restore_callbacks:\n\u001b[0;32m    418\u001b[0m   callback()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:398\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore.<locals>.restore_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device, saver \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_device_savers\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m    397\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m--> 398\u001b[0m     restore_ops\u001b[38;5;241m.\u001b[39mupdate(\u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (_, restore_fn) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_savers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    400\u001b[0m   restore_fn(file_prefix)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:112\u001b[0m, in \u001b[0;36m_SingleDeviceSaver.restore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    109\u001b[0m restore_ops \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m saveable, restored_tensors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_saveable_objects,\n\u001b[0;32m    111\u001b[0m                                       structured_restored_tensors):\n\u001b[1;32m--> 112\u001b[0m   restore_ops[saveable\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43msaveable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrestored_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestored_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py:128\u001b[0m, in \u001b[0;36mResourceVariableSaveable.restore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Copy the restored tensor to the variable's device.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_device):\n\u001b[1;32m--> 128\u001b[0m   restored_tensor \u001b[38;5;241m=\u001b[39m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestored_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     assigned_variable \u001b[38;5;241m=\u001b[39m resource_variable_ops\u001b[38;5;241m.\u001b[39mshape_safe_assign_variable_handle(\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_op, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_shape, restored_tensor)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]"
     ]
    }
   ],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "\n",
    "# Contains input token ids\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "# Contains input mask values\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "# Contains input type (whether token belongs to sequence A or B) values\n",
    "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"input_type_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True)\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "\n",
    "hub_encoder = tf.keras.models.Model(\n",
    "    inputs={\"input_word_ids\": input_word_ids, \"input_mask\": input_mask, \"segment_ids\": input_type_ids}, \n",
    "    outputs=[pooled_output, sequence_output]\n",
    ")\n",
    "\n",
    "# Check the outputs of the Bert layer\n",
    "print(pooled_output.shape)\n",
    "print(sequence_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasLayer' object has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbert_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KerasLayer' object has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "bert_layer.inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file is at: b'C:\\\\Users\\\\thush\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\ce53fe6769d2ac3a260e92555120c54e1aecbea6\\\\assets\\\\vocab.txt'\n",
      "Configuration do_lower_case of BERT: True\n"
     ]
    }
   ],
   "source": [
    "# Get the vocab file path from the BERT layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "print(\"Vocabulary file is at: {}\".format(vocab_file))\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "print(\"Configuration {} of BERT: {}\".format(\"do_lower_case\", do_lower_case))\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = tfm.nlp.layers.FastWordpieceBertTokenizer(vocab_file=vocab_file, lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding tokenization in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['She sells seashells by the seashore']\n",
      "Tokens IDs generated by BERT: [ 2016 15187 11915 18223  2015  2011  1996 11915 16892]\n",
      "Tokens generated by BERT: ['she', 'sells', 'seas', '##hell', '##s', 'by', 'the', 'seas', '##hore']\n"
     ]
    }
   ],
   "source": [
    "text = [\"She sells seashells by the seashore\"]\n",
    "print(\"Original text: {}\".format(text))\n",
    "#tokens = tokenizer.tokenize(text)\n",
    "tokens = tf.reshape(tokenizer(text), [-1])\n",
    "print(\"Tokens IDs generated by BERT: {}\".format(tokens))\n",
    "ids = [tokenizer._vocab[tid] for tid in tokens] \n",
    "#tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens generated by BERT: {}\".format(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens used by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS] has ID: 101\n",
      "Token: [SEP] has ID: 102\n",
      "Token: [MASK] has ID: 103\n",
      "Token: [PAD] has ID: 0\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]', '[MASK]', '[PAD]']\n",
    "ids = [tokenizer._vocab.index(tok) for tok in special_tokens]\n",
    "for t, i in zip(special_tokens, ids):\n",
    "    print(\"Token: {} has ID: {}\".format(t, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean       36.256856\n",
       "std        19.915669\n",
       "min        12.000000\n",
       "25%        18.000000\n",
       "50%        25.000000\n",
       "75%        56.000000\n",
       "90%        63.000000\n",
       "max        79.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13.2\n",
    "# Code listing 13.2\n",
    "def encode_sentence(s):\n",
    "    \"\"\" Encode a given sentence by tokenizing it and adding special tokens \"\"\"\n",
    "    \n",
    "    tokens = list(tf.reshape(tokenizer([\"CLS\" + s + \"[SEP]\"]), [-1]))\n",
    "    return tokens\n",
    "\n",
    "seq_lengths = pd.Series([len(encode_sentence(str(s))) for s in train_x])\n",
    "seq_lengths.describe(percentiles=[0.25, 0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the correct input format for BERT\n",
    "\n",
    "BERT model needs three inputs. These are formed into a dictionary having the following keys.\n",
    "\n",
    "* `input_word_ids` - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* `input_mask` - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* `input_type_ids` - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)\n",
    "\n",
    "`tf-models` library provides a convenient class to pack a given sentence (or a list of sentences) to this format called `BertPackInputs`. We'll use this to create our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_word_ids': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[  101,  2016, 15187, 11915, 18223,  2015,  2011,  1996, 11915,\n",
      "        16892,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]])>, 'input_mask': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'input_type_ids': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 60\n",
    "\n",
    "packer = tfm.nlp.layers.BertPackInputs(\n",
    "    seq_length=max_seq_length,\n",
    "    special_tokens_dict = tokenizer.get_special_tokens_dict()\n",
    ")\n",
    "\n",
    "text = [\"She sells seashells by the seashore\"]\n",
    "\n",
    "tok1 = tokenizer(text)\n",
    "\n",
    "packed = packer(tok1)\n",
    "\n",
    "print(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1094 training samples\n",
      "Added 200 validation samples\n",
      "Added 200 test samples\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Code listing 13.3\n",
    "def get_bert_inputs(tokenizer, docs,max_seq_len=None):\n",
    "    \"\"\" Generate inputs for BERT using a set of documents \"\"\"\n",
    "    \n",
    "    packer = tfm.nlp.layers.BertPackInputs(\n",
    "        seq_length=max_seq_length,\n",
    "        special_tokens_dict = tokenizer.get_special_tokens_dict()\n",
    "    )\n",
    "    \n",
    "    packed = packer(tokenizer(docs))\n",
    "    \n",
    "    packed_numpy = dict([(k, v.numpy()) for k,v in packed.items()])\n",
    "    # Final output\n",
    "    return packed_numpy\n",
    "\n",
    "# Creating train/validation/test data\n",
    "train_inputs = get_bert_inputs(tokenizer, train_x, max_seq_len=60)\n",
    "print(f\"Added {len(train_inputs['input_word_ids'])} training samples\")\n",
    "\n",
    "valid_inputs = get_bert_inputs(tokenizer, valid_x, max_seq_len=60)\n",
    "print(f\"Added {len(valid_inputs['input_word_ids'])} validation samples\")\n",
    "\n",
    "test_inputs = get_bert_inputs(tokenizer, test_x, max_seq_len=60)\n",
    "print(f\"Added {len(test_inputs['input_word_ids'])} test samples\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Shuffling training data as a precaution\n",
    "train_inds = np.random.permutation(len(train_inputs[\"input_word_ids\"]))\n",
    "train_inputs = dict([(k, v[train_inds]) for k, v in train_inputs.items()])\n",
    "train_y = train_y[train_inds]\n",
    "\n",
    "print(train_y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a downstream classifier from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Config\n",
      "{'attention_dropout_rate': 0.1, 'dropout_rate': 0.1, 'hidden_activation': 'gelu', 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_layers': 12, 'type_vocab_size': 2, 'vocab_size': 30522}\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "import yaml\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/master/official/nlp/configs/models/bert_en_uncased_base.yaml\n",
    "with open(os.path.join(\"data\", \"bert_en_uncased_base.yaml\"), 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)['task']['model']['encoder']['bert']\n",
    "\n",
    "\n",
    "# Print BERT config\n",
    "print(\"BERT Config\")\n",
    "print(config_dict)\n",
    "\n",
    "encoder_config = tfm.nlp.encoders.EncoderConfig({\n",
    "    'type':'bert',\n",
    "    'bert': config_dict\n",
    "})\n",
    "\n",
    "\n",
    "#bert_encoder = tfm.nlp.encoders.build_encoder(encoder_config)\n",
    "\n",
    "\n",
    "# Generating a classifier and the encoder\n",
    "bert_classifier = tfm.nlp.models.BertClassifier(network=hub_encoder, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code listing 13.4\n",
    "# Set up epochs and steps\n",
    "epochs = 3\n",
    "batch_size = 1 #64\n",
    "eval_batch_size = 1 #64\n",
    "\n",
    "train_data_size = train_x.shape[0]\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(num_train_steps * 0.1)\n",
    "\n",
    "initial_learning_rate = 2e-6\n",
    "\n",
    "# Define the decay\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    end_learning_rate=0,\n",
    "    decay_steps=num_train_steps)\n",
    "\n",
    "# Define learning rate schedule\n",
    "warmup_schedule = tfm.optimization.lr_schedule.LinearWarmup(\n",
    "    warmup_learning_rate = 1e-10,\n",
    "    after_warmup_lr_sched = linear_decay,\n",
    "    warmup_steps = warmup_steps\n",
    ")\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = tf.keras.optimizers.experimental.Adam(\n",
    "    learning_rate = warmup_schedule\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BERT and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mbert_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:152\u001b[0m, in \u001b[0;36mZeros.__call__\u001b[1;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[0;32m    151\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mcall_with_layout(tf\u001b[38;5;241m.\u001b[39mzeros, layout, shape\u001b[38;5;241m=\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model\n",
    "bert_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Train the model\n",
    "bert_classifier.fit(\n",
    "      x=train_inputs, \n",
    "      y=train_y,\n",
    "      validation_data=(valid_inputs, valid_y),\n",
    "      validation_batch_size=eval_batch_size,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs\n",
    ")\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:HDF5 format does not save weights of `optimizer_experimental.Optimizer`, your optimizer will be recompiled at loading time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:HDF5 format does not save weights of `optimizer_experimental.Optimizer`, your optimizer will be recompiled at loading time.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(bert_classifier, os.path.join('models', 'bert_spam.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 87ms/step - loss: 0.5118 - accuracy: 0.7450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5118228197097778, 0.7450000047683716]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13.2\n",
    "bert_classifier.evaluate(test_inputs, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
