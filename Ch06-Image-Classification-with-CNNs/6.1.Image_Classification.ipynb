{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Teaching Machines to See\n",
    "\n",
    "## Image Classification\n",
    "\n",
    "In this, we will train a complex convolution neural network on an image classification dataset.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch06-Image-Classification-with-CNNs/6.1.Image_Classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import requests\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, AvgPool2D, Dense, Concatenate, Flatten, Lambda, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "random_seed = 4321\n",
    "# Fixing the random seed\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "\n",
    "For this chapter, we're going to use the tiny-imagenet dataset (200 categories of objects). This is a simplified version of the bigger and harder imagenet dataset (1000 categories of objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zip file already exists.\n"
     ]
    }
   ],
   "source": [
    "#Section 6.1 \n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','tiny-imagenet-200.zip')):\n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data','tiny-imagenet-200.zip'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "else:\n",
    "    print(\"The zip file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'tiny-imagenet-200')):\n",
    "    with zipfile.ZipFile(os.path.join('data','tiny-imagenet-200.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "else:\n",
    "    print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the label information of the data\n",
    "\n",
    "The classes in tiny-imagenet are coded by an id (known as `wnid` (WordNetID)). Here we will decode these IDs to get class descriptions of each ID, so we know what we're dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thush\\AppData\\Local\\Temp\\ipykernel_22676\\1569625529.py:16: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  wnids = pd.read_csv(wnids_path, header=None, squeeze=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wnid</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02124075</td>\n",
       "      <td>Egyptian cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n04067472</td>\n",
       "      <td>reel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n04540053</td>\n",
       "      <td>volleyball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n04099969</td>\n",
       "      <td>rocking chair, rocker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n07749582</td>\n",
       "      <td>lemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n01641577</td>\n",
       "      <td>bullfrog, Rana catesbeiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n02802426</td>\n",
       "      <td>basketball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n09246464</td>\n",
       "      <td>cliff, drop, drop-off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n07920052</td>\n",
       "      <td>espresso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n03970156</td>\n",
       "      <td>plunger, plumber's helper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>n03891332</td>\n",
       "      <td>parking meter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n02106662</td>\n",
       "      <td>German shepherd, German shepherd dog, German p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>n03201208</td>\n",
       "      <td>dining table, board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n02279972</td>\n",
       "      <td>monarch, monarch butterfly, milkweed butterfly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>n02132136</td>\n",
       "      <td>brown bear, bruin, Ursus arctos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>n04146614</td>\n",
       "      <td>school bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>n07873807</td>\n",
       "      <td>pizza, pizza pie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>n02364673</td>\n",
       "      <td>guinea pig, Cavia cobaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>n04507155</td>\n",
       "      <td>umbrella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>n03854065</td>\n",
       "      <td>organ, pipe organ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>n03838899</td>\n",
       "      <td>oboe, hautboy, hautbois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>n03733131</td>\n",
       "      <td>maypole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>n01443537</td>\n",
       "      <td>goldfish, Carassius auratus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>n07875152</td>\n",
       "      <td>potpie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>n03544143</td>\n",
       "      <td>hourglass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         wnid                                              class\n",
       "0   n02124075                                       Egyptian cat\n",
       "1   n04067472                                               reel\n",
       "2   n04540053                                         volleyball\n",
       "3   n04099969                              rocking chair, rocker\n",
       "4   n07749582                                              lemon\n",
       "5   n01641577                         bullfrog, Rana catesbeiana\n",
       "6   n02802426                                         basketball\n",
       "7   n09246464                              cliff, drop, drop-off\n",
       "8   n07920052                                           espresso\n",
       "9   n03970156                          plunger, plumber's helper\n",
       "10  n03891332                                      parking meter\n",
       "11  n02106662  German shepherd, German shepherd dog, German p...\n",
       "12  n03201208                                dining table, board\n",
       "13  n02279972  monarch, monarch butterfly, milkweed butterfly...\n",
       "14  n02132136                    brown bear, bruin, Ursus arctos\n",
       "15  n04146614                                         school bus\n",
       "16  n07873807                                   pizza, pizza pie\n",
       "17  n02364673                           guinea pig, Cavia cobaya\n",
       "18  n04507155                                           umbrella\n",
       "19  n03854065                                  organ, pipe organ\n",
       "20  n03838899                            oboe, hautboy, hautbois\n",
       "21  n03733131                                            maypole\n",
       "22  n01443537                        goldfish, Carassius auratus\n",
       "23  n07875152                                             potpie\n",
       "24  n03544143                                          hourglass"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Section 6.1\n",
    "#Code listing 6.1\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# The file paths we're going to use\n",
    "data_dir = os.path.join('data', 'tiny-imagenet-200')\n",
    "wnids_path = os.path.join(data_dir, 'wnids.txt')\n",
    "words_path = os.path.join(data_dir, 'words.txt')\n",
    "\n",
    "def get_tiny_imagenet_classes(wnids_path, words_path):\n",
    "    \n",
    "    # Read the csv files\n",
    "    # wninds.txt contains the wnids of the data in the dataset\n",
    "    wnids = pd.read_csv(wnids_path, header=None, squeeze=True)\n",
    "    # words.txt contains a mapping from wnid to the class description\n",
    "    words = pd.read_csv(words_path, sep='\\t', index_col=0, header=None)\n",
    "    # Get only the class descriptions corresponding to the wnids in the dataset\n",
    "    words_200 = words.loc[wnids].rename({1:'class'}, axis=1)\n",
    "    \n",
    "    words_200.index.name = 'wnid'\n",
    "    return words_200.reset_index()\n",
    "\n",
    "labels = get_tiny_imagenet_classes(wnids_path, words_path)\n",
    "labels.head(n=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many training instances for each class are there?\n",
    "\n",
    "**Note**: If you run this after separating out the validation data, you'll see 450 instead of 500 in the `n_train` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wnid</th>\n",
       "      <th>class</th>\n",
       "      <th>n_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02124075</td>\n",
       "      <td>Egyptian cat</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n04067472</td>\n",
       "      <td>reel</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n04540053</td>\n",
       "      <td>volleyball</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n04099969</td>\n",
       "      <td>rocking chair, rocker</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n07749582</td>\n",
       "      <td>lemon</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        wnid                  class  n_train\n",
       "0  n02124075           Egyptian cat      500\n",
       "1  n04067472                   reel      500\n",
       "2  n04540053             volleyball      500\n",
       "3  n04099969  rocking chair, rocker      500\n",
       "4  n07749582                  lemon      500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Section 6.1\n",
    "\n",
    "import os\n",
    "def get_image_count(data_dir):  \n",
    "    \"\"\" Counts the number of jpeg files in a given folder\"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        return 0\n",
    "    return len([f for f in os.listdir(data_dir) if f.lower().endswith('jpeg')])\n",
    "    \n",
    "# Here we use the apply function in conjunction with the get_image_count to get the count of images in each\n",
    "# subfolder in the train directory\n",
    "labels[\"n_train\"] = labels[\"wnid\"].apply(lambda x: get_image_count(os.path.join(data_dir, 'train', x, 'images')))\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at summary statistics of the `n_train` column\n",
    "\n",
    "Summary statistics is a great way to get an instant view of a column in a dataframe. It gives us important information like mean/standard deviation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200.0\n",
       "mean     500.0\n",
       "std        0.0\n",
       "min      500.0\n",
       "25%      500.0\n",
       "50%      500.0\n",
       "75%      500.0\n",
       "max      500.0\n",
       "Name: n_train, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[\"n_train\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the image dimensions in the dataset\n",
    "\n",
    "Before moving on to modelling we need to understand the image dimensions (height and width). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12500.0</td>\n",
       "      <td>12500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         width   height\n",
       "count  12500.0  12500.0\n",
       "mean      64.0     64.0\n",
       "std        0.0      0.0\n",
       "min       64.0     64.0\n",
       "25%       64.0     64.0\n",
       "50%       64.0     64.0\n",
       "75%       64.0     64.0\n",
       "max       64.0     64.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Section 6.1\n",
    "#Code listing 6.2\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# A list that will hold image height and width information\n",
    "image_sizes = []\n",
    "\n",
    "# We will only look at the first 25 directories (to save time)\n",
    "for wnid in labels[\"wnid\"].iloc[:25]:\n",
    "    img_dir = os.path.join('data', 'tiny-imagenet-200', 'train', wnid, 'images')\n",
    "    for f in os.listdir(img_dir):\n",
    "        # Only read the file if ends with JPEG\n",
    "        if f.endswith('JPEG'):\n",
    "            # Append the height and width to the list\n",
    "            # e.g. [(img1.width, img1.height), (img2.width, img2.height), ...]\n",
    "            image_sizes.append(Image.open(os.path.join(img_dir, f)).size)\n",
    "\n",
    "# Using the format of image_sizes, we can directly create a dataframe\n",
    "img_df = pd.DataFrame.from_records(image_sizes)\n",
    "img_df.columns = [\"width\", \"height\"]\n",
    "# Getting summary statistics of all columns\n",
    "img_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Getting information about pixel values of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.100000</td>\n",
       "      <td>-1.350000</td>\n",
       "      <td>121.372945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.383481</td>\n",
       "      <td>1.565248</td>\n",
       "      <td>40.301543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>63.183757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>98.160726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>112.580322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>127.742086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>227.127360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Min        Max        Mean\n",
       "count  20.000000  20.000000   20.000000\n",
       "mean    2.100000  -1.350000  121.372945\n",
       "std     7.383481   1.565248   40.301543\n",
       "min     0.000000  -8.000000   63.183757\n",
       "25%     0.000000  -1.000000   98.160726\n",
       "50%     0.000000  -1.000000  112.580322\n",
       "75%     0.000000  -1.000000  127.742086\n",
       "max    32.000000  -1.000000  227.127360"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "images = []\n",
    "for wnid in labels[\"wnid\"].iloc[:2]:\n",
    "    img_dir = os.path.join('data', 'tiny-imagenet-200', 'train', wnid, 'images')\n",
    "    for f in os.listdir(img_dir)[:10]:\n",
    "        if f.endswith('JPEG'):\n",
    "            img = np.array(Image.open(os.path.join(img_dir, f)))\n",
    "            images.append((img.min(), img.max(), img.mean()))\n",
    "\n",
    "img_df = pd.DataFrame.from_records(images)\n",
    "img_df.columns = [\"Min\", \"Max\", \"Mean\"]\n",
    "img_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the data generators\n",
    "\n",
    "In the dataset, we currently have a,\n",
    "\n",
    "* Training set: `train` directory\n",
    "* Testing set: `val` directory\n",
    "\n",
    "But to properly train a model we need three datasets,\n",
    "\n",
    "* Training set - Used to train the model\n",
    "* Validation set - Used to continuously monitor model performance while training\n",
    "* Testing set - Used to test the model, only after training finishes\n",
    "\n",
    "Therefore, we will separate a 10% from training data and feed this data through as a separate generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 200 classes.\n",
      "Found 10000 validated image filenames belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "#Section 6.2\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def get_test_labels_df(test_labels_path):\n",
    "    \"\"\" Reading the test data labels for all files in the test set as a data frame \"\"\"\n",
    "    test_df = pd.read_csv(test_labels_path, sep='\\t', index_col=None, header=None)\n",
    "    test_df = test_df.iloc[:,[0,1]].rename({0:\"filename\", 1:\"class\"}, axis=1)\n",
    "    return test_df\n",
    "\n",
    "def get_train_valid_test_data_generators(batch_size, target_size):\n",
    "    # Define a Keras ImageDataGenerator with image centering (subtract mean)\n",
    "    \n",
    "    image_gen = ImageDataGenerator(samplewise_center=True, validation_split=0.1)\n",
    "\n",
    "    # Define a training data generator\n",
    "    partial_flow_func = partial(\n",
    "        image_gen.flow_from_directory, \n",
    "        directory=os.path.join('data','tiny-imagenet-200', 'train'), \n",
    "        target_size=target_size, classes=None,\n",
    "        class_mode='categorical', batch_size=batch_size, \n",
    "        shuffle=True, seed=random_seed)\n",
    "    \n",
    "    # Get the training data subset\n",
    "    train_gen = partial_flow_func(subset='training')\n",
    "    # Get the validation data subset\n",
    "    valid_gen = partial_flow_func(subset='validation')\n",
    "    \n",
    "    \n",
    "    # Define a testing data generator\n",
    "    # This function uses flow_from_dataframe instead of flow_from_directory\n",
    "    test_df = get_test_labels_df(os.path.join('data','tiny-imagenet-200',  'val', 'val_annotations.txt'))\n",
    "    test_gen = image_gen.flow_from_dataframe(\n",
    "        test_df, directory=os.path.join('data','tiny-imagenet-200',  'val', 'images'), target_size=target_size, classes=None,\n",
    "        class_mode='categorical', batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    return train_gen, valid_gen, test_gen\n",
    "\n",
    "def data_gen_aux(gen):    \n",
    "    # We need to modify our standard data generators to output the same target three times\n",
    "    for x,y in gen:        \n",
    "        yield x,(y,y,y)\n",
    "\n",
    "# The model was really struggling with batch size 32\n",
    "batch_size = 8\n",
    "target_size = (56, 56)\n",
    "# Getting the train,valid, test data generators\n",
    "train_gen, valid_gen, test_gen = get_train_valid_test_data_generators(batch_size, target_size)\n",
    "# Modifying the data generators to fit the model targets\n",
    "train_gen_aux = data_gen_aux(train_gen)\n",
    "valid_gen_aux = data_gen_aux(valid_gen)\n",
    "test_gen_aux = data_gen_aux(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the consistency of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful! Labels across all trials were consistent.\n"
     ]
    }
   ],
   "source": [
    "from itertools import tee\n",
    "all_labels = []\n",
    "n_trials = 10\n",
    "\n",
    "valid_gen_test = tee(valid_gen, n_trials)\n",
    "\n",
    "for i in range(n_trials):    \n",
    "    labels = []\n",
    "    for j in range(5):\n",
    "        _, ohe = next(valid_gen_test[i])\n",
    "        # Convert one hot encoded to class labels\n",
    "        labels.append(np.argmax(ohe, axis=-1))\n",
    "        \n",
    "    # Concat all labels\n",
    "    labels = np.reshape(np.concatenate(labels, axis=0), (1,-1))        \n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Concat all labels accross all trials\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Assert the labels are equal across all trials\n",
    "assert np.all(np.all(all_labels == all_labels[0,:], axis=0)), \"Labels across multiple trials were not equal\"\n",
    "print(\"Successful! Labels across all trials were consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Inception net v1\n",
    "\n",
    "Here we will be creating the Inception net v1 model using Keras Functional API\n",
    "\n",
    "### Funtions that encapsulate various components of the Inception net v1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Section 6.3\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, AvgPool2D, Dense, Concatenate, Flatten, Lambda, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Code listing 6.3\n",
    "def stem(inp):\n",
    "    conv1 = Conv2D(64, (7,7), strides=(1,1), activation='relu', padding='same')(inp)\n",
    "    maxpool2 = MaxPool2D((3,3), strides=(2,2), padding='same')(conv1)\n",
    "    lrn3 = Lambda(lambda x: tf.nn.local_response_normalization(x))(maxpool2)\n",
    "\n",
    "    conv4 = Conv2D(64, (1,1), strides=(1,1), padding='same')(lrn3)\n",
    "    conv5 = Conv2D(192, (3,3), strides=(1,1), activation='relu', padding='same')(conv4)\n",
    "    lrn6 = Lambda(lambda x: tf.nn.local_response_normalization(x))(conv5)\n",
    "\n",
    "    maxpool7 = MaxPool2D((3,3), strides=(1,1), padding='same')(lrn6)\n",
    "\n",
    "    return maxpool7\n",
    "\n",
    "# Code listing 6.4\n",
    "def inception(inp, n_filters):\n",
    "\n",
    "    # 1x1 layer\n",
    "    # init argument defaults to glorot_uniform\n",
    "    out1 = Conv2D(n_filters[0][0], (1,1), strides=(1,1), activation='relu', padding='same')(inp)\n",
    "\n",
    "    # 1x1 followed by 3x3\n",
    "    out2_1 = Conv2D(n_filters[1][0], (1,1), strides=(1,1), activation='relu', padding='same')(inp)\n",
    "    out2_2 = Conv2D(n_filters[1][1], (3,3), strides=(1,1), activation='relu', padding='same')(out2_1)\n",
    "\n",
    "    # 1x1 followed by 5x5\n",
    "    out3_1 = Conv2D(n_filters[2][0], (1,1), strides=(1,1), activation='relu', padding='same')(inp)\n",
    "    out3_2 = Conv2D(n_filters[2][1], (5,5), strides=(1,1), activation='relu', padding='same')(out3_1)\n",
    "\n",
    "    # 3x3 (pool) followed by 1x1\n",
    "    out4_1 = MaxPool2D((3,3), strides=(1,1), padding='same')(inp)\n",
    "    out4_2 = Conv2D(n_filters[3][0], (1,1), strides=(1,1), activation='relu', padding='same')(out4_1)\n",
    "\n",
    "    out = Concatenate(axis=-1)([out1, out2_2, out3_2, out4_2])\n",
    "    return out\n",
    "\n",
    "# Code listing 6.5\n",
    "def aux_out(inp,name=None):    \n",
    "    avgpool1 = AvgPool2D((5,5), strides=(3,3), padding='valid')(inp)\n",
    "    conv1 = Conv2D(128, (1,1), activation='relu', padding='same')(avgpool1)\n",
    "    flat = Flatten()(conv1)\n",
    "    dense1 = Dense(1024, activation='relu')(flat)    \n",
    "    aux_out = Dense(200, activation='softmax', name=name)(dense1)\n",
    "    return aux_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 56, 56, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 56, 56, 64)   9472        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 28, 28, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 28, 28, 64)   0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 28, 28, 64)   4160        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 28, 192)  110784      ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 28, 28, 192)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 192)  0          ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 28, 28, 96)   18528       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 28, 28, 16)   3088        ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 192)  0          ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 28, 28, 64)   12352       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 28, 28, 128)  110720      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 28, 28, 32)   12832       ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 28, 28, 32)   6176        ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 28, 28, 256)  0           ['conv2d_3[0][0]',               \n",
      "                                                                  'conv2d_5[0][0]',               \n",
      "                                                                  'conv2d_7[0][0]',               \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 28, 28, 128)  32896       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 28, 28, 32)   8224        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 256)  0          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 28, 28, 128)  32896       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 28, 28, 192)  221376      ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 28, 28, 96)   76896       ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 28, 28, 64)   16448       ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 28, 28, 480)  0           ['conv2d_9[0][0]',               \n",
      "                                                                  'conv2d_11[0][0]',              \n",
      "                                                                  'conv2d_13[0][0]',              \n",
      "                                                                  'conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 480)  0          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 14, 14, 96)   46176       ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 14, 14, 16)   7696        ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 14, 14, 480)  0          ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 14, 14, 192)  92352       ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 14, 14, 208)  179920      ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 14, 14, 48)   19248       ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 14, 14, 64)   30784       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 14, 14, 512)  0           ['conv2d_15[0][0]',              \n",
      "                                                                  'conv2d_17[0][0]',              \n",
      "                                                                  'conv2d_19[0][0]',              \n",
      "                                                                  'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 14, 14, 112)  57456       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 14, 14, 24)   12312       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 14, 14, 512)  0          ['concatenate_2[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 14, 14, 160)  82080       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 14, 14, 224)  226016      ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 14, 14, 64)   38464       ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 14, 14, 64)   32832       ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 14, 14, 512)  0           ['conv2d_21[0][0]',              \n",
      "                                                                  'conv2d_23[0][0]',              \n",
      "                                                                  'conv2d_25[0][0]',              \n",
      "                                                                  'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 14, 14, 128)  65664       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 14, 14, 24)   12312       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 512)  0          ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 14, 14, 128)  65664       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 14, 14, 256)  295168      ['conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 14, 14, 64)   38464       ['conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 14, 14, 64)   32832       ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 14, 14, 512)  0           ['conv2d_28[0][0]',              \n",
      "                                                                  'conv2d_30[0][0]',              \n",
      "                                                                  'conv2d_32[0][0]',              \n",
      "                                                                  'conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 14, 14, 144)  73872       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 14, 14, 32)   16416       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 14, 14, 512)  0          ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 14, 14, 112)  57456       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 14, 14, 288)  373536      ['conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 14, 14, 64)   51264       ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 14, 14, 64)   32832       ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 14, 14, 528)  0           ['conv2d_34[0][0]',              \n",
      "                                                                  'conv2d_36[0][0]',              \n",
      "                                                                  'conv2d_38[0][0]',              \n",
      "                                                                  'conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 14, 14, 160)  84640       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 14, 14, 32)   16928       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPooling2D)  (None, 14, 14, 528)  0          ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 14, 14, 256)  135424      ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 14, 14, 320)  461120      ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 14, 14, 128)  102528      ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 14, 14, 128)  67712       ['max_pooling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 14, 14, 832)  0           ['conv2d_40[0][0]',              \n",
      "                                                                  'conv2d_42[0][0]',              \n",
      "                                                                  'conv2d_44[0][0]',              \n",
      "                                                                  'conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (None, 7, 7, 832)   0           ['concatenate_6[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 7, 7, 160)    133280      ['max_pooling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 7, 7, 32)     26656       ['max_pooling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooling2D  (None, 7, 7, 832)   0           ['max_pooling2d_10[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 7, 7, 256)    213248      ['max_pooling2d_10[0][0]']       \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_49 (Conv2D)             (None, 7, 7, 320)    461120      ['conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 7, 7, 128)    102528      ['conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 7, 7, 128)    106624      ['max_pooling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 7, 7, 832)    0           ['conv2d_47[0][0]',              \n",
      "                                                                  'conv2d_49[0][0]',              \n",
      "                                                                  'conv2d_51[0][0]',              \n",
      "                                                                  'conv2d_52[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 7, 7, 192)    159936      ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 7, 7, 48)     39984       ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooling2D  (None, 7, 7, 832)   0           ['concatenate_7[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 7, 7, 384)    319872      ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 7, 7, 384)    663936      ['conv2d_54[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 7, 7, 128)    153728      ['conv2d_56[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 7, 7, 128)    106624      ['max_pooling2d_12[0][0]']       \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 4, 4, 512)   0           ['concatenate_2[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 4, 4, 528)   0           ['concatenate_5[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 7, 7, 1024)   0           ['conv2d_53[0][0]',              \n",
      "                                                                  'conv2d_55[0][0]',              \n",
      "                                                                  'conv2d_57[0][0]',              \n",
      "                                                                  'conv2d_58[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 4, 4, 128)    65664       ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 4, 4, 128)    67712       ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 1, 1, 1024)  0           ['concatenate_8[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2048)         0           ['conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 1024)         0           ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         2098176     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         2098176     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " final (Dense)                  (None, 200)          205000      ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " aux1 (Dense)                   (None, 200)          205000      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " aux2 (Dense)                   (None, 200)          205000      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,918,280\n",
      "Trainable params: 10,918,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 6.3\n",
    "# Code listing 6.6\n",
    "def inception_v1():\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    inp = Input(shape=(56,56,3))\n",
    "    stem_out = stem(inp)\n",
    "    inc_3a = inception(stem_out, [(64,),(96,128),(16,32),(32,)])\n",
    "    inc_3b = inception(inc_3a, [(128,),(128,192),(32,96),(64,)])\n",
    "\n",
    "    maxpool = MaxPool2D((3,3), strides=(2,2), padding='same')(inc_3b)\n",
    "\n",
    "    inc_4a = inception(maxpool, [(192,),(96,208),(16,48),(64,)])\n",
    "    inc_4b = inception(inc_4a, [(160,),(112,224),(24,64),(64,)])\n",
    "\n",
    "    aux_out1 = aux_out(inc_4a, name='aux1')\n",
    "\n",
    "    inc_4c = inception(inc_4b, [(128,),(128,256),(24,64),(64,)])\n",
    "    inc_4d = inception(inc_4c, [(112,),(144,288),(32,64),(64,)])\n",
    "    inc_4e = inception(inc_4d, [(256,),(160,320),(32,128),(128,)])\n",
    "    \n",
    "    maxpool = MaxPool2D((3,3), strides=(2,2), padding='same')(inc_4e)\n",
    "    \n",
    "    aux_out2 = aux_out(inc_4d, name='aux2')\n",
    "\n",
    "    inc_5a = inception(maxpool, [(256,),(160,320),(32,128),(128,)])\n",
    "    inc_5b = inception(inc_5a, [(384,),(192,384),(48,128),(128,)])\n",
    "    avgpool1 = AvgPool2D((7,7), strides=(1,1), padding='valid')(inc_5b)\n",
    "\n",
    "    flat_out = Flatten()(avgpool1)\n",
    "    out_main = Dense(200, activation='softmax', name='final')(flat_out)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[out_main, aux_out1, aux_out2])\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                       optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = inception_v1()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6.4\n",
    "\n",
    "def get_steps_per_epoch(n_data, batch_size):\n",
    "    \"\"\" Given the data size and batch size, gives the number of steps to travers the full dataset \"\"\"\n",
    "    if n_data%batch_size==0:\n",
    "        return int(n_data/batch_size)\n",
    "    else:\n",
    "        return int(n_data*1.0/batch_size)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   77/11250 [..............................] - ETA: 8:15 - loss: 15.9178 - final_loss: 5.3046 - aux1_loss: 5.3090 - aux2_loss: 5.3042 - final_accuracy: 0.0016 - aux1_accuracy: 0.0016 - aux2_accuracy: 0.0016"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Starting time\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model, not how we are specifying steps_per_epoch and validation_steps\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# to prevent the model from training forever\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# We are also using the data generators (not actual data loaded to memory) to train the model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_gen_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_steps_per_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_steps_per_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Ending time\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Print time it took\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section 6.4\n",
    "# Code listing 6.7\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create a directory called eval which stores model performance\n",
    "if not os.path.exists('eval'):\n",
    "    os.mkdir('eval')\n",
    "    \n",
    "# This will automatically log model performan to this file\n",
    "csv_logger = CSVLogger(os.path.join('eval','1_eval_base.log'))\n",
    "    \n",
    "t1 = time.time() # Starting time\n",
    "\n",
    "# Train the model, not how we are specifying steps_per_epoch and validation_steps\n",
    "# to prevent the model from training forever\n",
    "# We are also using the data generators (not actual data loaded to memory) to train the model\n",
    "history = model.fit(\n",
    "    train_gen_aux, validation_data=valid_gen_aux, \n",
    "    steps_per_epoch=get_steps_per_epoch(0.9*500*200,batch_size), \n",
    "    validation_steps=get_steps_per_epoch(0.1*500*200,batch_size),\n",
    "    epochs=50, callbacks=[csv_logger]\n",
    ")\n",
    "t2 = time.time() # Ending time\n",
    "\n",
    "# Print time it took\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Metrics of the model\n",
    "\n",
    "This is a multi output model. So it pays off to check what are the metrics that the model uses. You can get metric names by calling `model.metrics_names` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'final_loss', 'aux1_loss', 'aux2_loss', 'final_accuracy', 'aux1_accuracy', 'aux2_accuracy']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to models directory\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir(\"models\")\n",
    "model.save(os.path.join('models', 'inception_v1_base.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test accuracy of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 62s 20ms/step - loss: 15.8966 - final_loss: 5.2988 - aux1_loss: 5.2989 - aux2_loss: 5.2989 - final_accuracy: 0.0051 - aux1_accuracy: 0.0051 - aux2_accuracy: 0.0051\n",
      "{'loss': 15.89664077758789, 'final_loss': 5.29881477355957, 'aux1_loss': 5.298887729644775, 'aux2_loss': 5.298939228057861, 'final_accuracy': 0.005119999870657921, 'aux1_accuracy': 0.005119999870657921, 'aux2_accuracy': 0.005119999870657921}\n"
     ]
    }
   ],
   "source": [
    "# Section 6.4\n",
    "\n",
    "# Load the model from disk\n",
    "model = load_model(os.path.join('models','inception_v1_base.h5'))\n",
    "\n",
    "# Evaluate the model\n",
    "test_res = model.evaluate(test_gen_aux, steps=get_steps_per_epoch(500*50, batch_size))\n",
    "\n",
    "# Print the results as a dictionary {<metric name>: <value>}\n",
    "test_res_dict = dict(zip(model.metrics_names, test_res))\n",
    "print(test_res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
