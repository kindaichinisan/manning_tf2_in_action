{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling (Natural Language Processing)\n",
    "\n",
    "Natural Language Processing (NLP) is a vast subject with many different specializations. Here we are going to discuss a topic that gave rise to ground breaking models like BERT that changed the NLP landscape dramatically; language modelling. Language modelling is an unsupervised training method, where you ask a model to predict the next character/word/sentence given the previous characters/words/sentences.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch10-NLP-with-TF2-Language-Modelling/10.1.Language_modelling.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import requests\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the bAbI's children story dataset\n",
    "\n",
    "For this task, we'll be using a popular children story dataset from the [bAbI project](https://research.fb.com/downloads/babi/) of Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Section 10.1\n",
    "\n",
    "# Code listing 10.1\n",
    "\n",
    "# Downloading the data\n",
    "# http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'lm','CBTest.tgz')):\n",
    "    url = \"http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists(os.path.join('data','lm')):\n",
    "        os.makedirs(os.path.join('data','lm'))\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'lm', 'CBTest.tgz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'lm', 'CBTest')):\n",
    "    # Write to a file\n",
    "    tarf = tarfile.open(os.path.join(\"data\",\"lm\",\"CBTest.tgz\"))\n",
    "    tarf.extractall(os.path.join(\"data\",\"lm\"))  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "After downloading the data, let's read that into memory. There are three sets; training validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code listing 10.2\n",
    "\n",
    "def read_data(path):\n",
    "    stories = []\n",
    "\n",
    "    with open(path, 'r') as f:    \n",
    "        s = [] \n",
    "        for row in f:\n",
    "            \n",
    "            if row.startswith(\"_BOOK_TITLE_\"):\n",
    "                if len(s)>0:\n",
    "                    stories.append(' '.join(s).lower())            \n",
    "                s = []           \n",
    "\n",
    "            s.append(row)\n",
    "            \n",
    "    if len(s)>0:\n",
    "        stories.append(' '.join(s).lower())  \n",
    "    \n",
    "    return stories\n",
    "\n",
    "stories = read_data(os.path.join('data','lm','CBTest','data','cbt_train.txt'))\n",
    "val_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_valid.txt'))\n",
    "test_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 98 stories (train)\n",
      "Collected 5 stories (valid)\n",
      "Collected 5 stories (test)\n",
      "_book_title_ : andrew_lang___prince_prigio.txt.out\n",
      " chapter i. -lcb- chapter heading picture : p1.jp\n",
      "\n",
      " _book_title_ : andrew_lang___the_violet_fairy_book.txt.out\n",
      " a tale of the tontlawald long , long ago\n"
     ]
    }
   ],
   "source": [
    "print(\"Collected {} stories (train)\".format(len(stories)))\n",
    "print(\"Collected {} stories (valid)\".format(len(val_stories)))\n",
    "print(\"Collected {} stories (test)\".format(len(test_stories)))\n",
    "print(stories[0][:100])\n",
    "print('\\n', stories[10][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99464\n",
      "99834\n",
      "136758\n",
      "761257\n",
      "524783\n",
      "522998\n",
      "528840\n",
      "531058\n",
      "527601\n",
      "674648\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(stories[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick drive to vocabulary-ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",      348650\n",
      "the    242890\n",
      ".\\n    192549\n",
      "and    179205\n",
      "to     120821\n",
      "a      101990\n",
      "of      96748\n",
      "i       79780\n",
      "he      78129\n",
      "was     66593\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 14473\n"
     ]
    }
   ],
   "source": [
    "# Section 10.1\n",
    "\n",
    "from collections import Counter\n",
    "# Create a large list which contains all the words in all the reviews\n",
    "data_list = [w for doc in stories for w in doc.split(' ')]\n",
    "\n",
    "# Create a Counter object from that list\n",
    "# Counter returns a dictionary, where key is a word and the value is the frequency\n",
    "cnt = Counter(data_list)\n",
    "\n",
    "# Convert the result to a pd.Series \n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "\n",
    "# Print most common words\n",
    "print(freq_df.head(n=10))\n",
    "\n",
    "# Count of words >= n frequent\n",
    "n=10\n",
    "print(\"\\nVocabulary size (>={} frequent): {}\".format(n, (freq_df>=n).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert strings to n-grams\n",
    "\n",
    "For our language modelling task, we're going to split strings into bigrams. That is, given the string\n",
    "\n",
    "`i went to the office`, it is converted to,\n",
    "\n",
    "`[\"i \", \"we\", \"nt\", \" t\", \"o \", \"th\", \"e \", \"of\", \"fi\", \"ce\"]`\n",
    "\n",
    "We will also look at what are the most common bigrams and some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I like chocolates\n",
      "\t1-grams: ['I', ' ', 'l', 'i', 'k', 'e', ' ', 'c', 'h', 'o', 'c', 'o', 'l', 'a', 't', 'e', 's']\n",
      "\t2-grams: ['I ', 'li', 'ke', ' c', 'ho', 'co', 'la', 'te', 's']\n",
      "\t3-grams: ['I l', 'ike', ' ch', 'oco', 'lat', 'es']\n",
      "\n",
      "Sample of most-common bigrams\n",
      "e     455663\n",
      " t    344271\n",
      "he    310016\n",
      "d     309397\n",
      "th    284413\n",
      " a    267776\n",
      "t     258050\n",
      "s     228165\n",
      " h    192929\n",
      " s    183213\n",
      "dtype: int64\n",
      "\n",
      "Median: 135.0\n",
      "\n",
      "count      1079.000000\n",
      "mean      12050.894347\n",
      "std       36289.960299\n",
      "min           1.000000\n",
      "25%           4.000000\n",
      "50%         135.000000\n",
      "75%        6253.000000\n",
      "90%       34156.800000\n",
      "max      455663.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    \"\"\" This function takes a given string and split it into desired sized n-grams \"\"\"\n",
    "    return [text[i:i+n] for i in range(0,len(text),n)]\n",
    "\n",
    "# Test the ngrams function with a variety of ngrams\n",
    "test_string = \"I like chocolates\"\n",
    "print(\"Original: {}\".format(test_string))\n",
    "for i in list(range(3)):\n",
    "    print(\"\\t{}-grams: {}\".format(i+1, get_ngrams(test_string, i+1)))\n",
    "\n",
    "# Create a counter with the bi-grams\n",
    "ngrams = 2\n",
    "\n",
    "text = chain(*[get_ngrams(s, ngrams) for s in stories])\n",
    "cnt = Counter(text)\n",
    "\n",
    "# Create a pandas series with the counter results\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "print(\"\\nSample of most-common bigrams\")\n",
    "print(freq_df.head(n=10))\n",
    "print(\"\\nMedian: {}\\n\".format(freq_df.median()))\n",
    "# Get summary statistics\n",
    "print(freq_df.describe(percentiles=[0.25,0.5,0.75,0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the vocabulary\n",
    "\n",
    "We will set the vocabulary size to the number of words (bi-grams) that appear at least 10 times in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 741\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df>=10).sum()\n",
    "print(\"Size of vocabulary: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-grams to IDs: Defining a Keras tokenizer\n",
    "\n",
    "Here, we're going to fit a tokenizer on the train data in order to convert bi-grams to IDs. The tokenizer will assign a specific ID to each unique bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.1\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer for the determined vocabulary size\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "\n",
    "# Get ngrams in the training data\n",
    "train_ngram_stories = [get_ngrams(s,ngrams) for s in stories]\n",
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(train_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for training data\n",
    "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for validation data\n",
    "val_ngram_stories = [get_ngrams(s,ngrams) for s in val_stories]\n",
    "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for testing data\n",
    "test_ngram_stories = [get_ngrams(s,ngrams) for s in test_stories]\n",
    "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some word ID sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: _book_title_ : andrew_lang___the_yellow_fairy_book\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' a', 'nd', 're', 'w_', 'la', 'ng', '__', '_t', 'he', '_y', 'el', 'lo', 'w_', 'fa', 'ir', 'y_', 'bo', 'ok']\n",
      "Word ID sequence: [549, 97, 554, 102, 173, 537, 325, 7, 22, 25, 708, 112, 37, 524, 593, 4, 1, 81, 109, 708, 170, 133, 583, 161, 194]\n",
      "\n",
      "\n",
      "Original: _book_title_ : lewis_carroll___alice's_adventures_\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' l', 'ew', 'is', '_c', 'ar', 'ro', 'll', '__', '_a', 'li', 'ce', \"'s\", '_a', 'dv', 'en', 'tu', 're', 's_']\n",
      "Word ID sequence: [549, 97, 554, 102, 173, 537, 325, 52, 227, 49, 718, 53, 75, 54, 524, 1, 74, 120, 182, 1, 418, 38, 223, 25, 614]\n",
      "\n",
      "\n",
      "Original: _book_title_ : lucy_maud_montgomery___lucy_maud_mo\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' l', 'uc', 'y_', 'ma', 'ud', '_m', 'on', 'tg', 'om', 'er', 'y_', '__', 'lu', 'cy', '_m', 'au', 'd_', 'mo']\n",
      "Word ID sequence: [549, 97, 554, 102, 173, 537, 325, 52, 229, 583, 105, 296, 641, 43, 565, 100, 16, 583, 524, 289, 398, 641, 211, 673, 147]\n",
      "\n",
      "\n",
      "Original: _book_title_ : rudyard_kipling___the_jungle_book.t\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' r', 'ud', 'ya', 'rd', '_k', 'ip', 'li', 'ng', '__', '_t', 'he', '_j', 'un', 'gl', 'e_', 'bo', 'ok', '.t']\n",
      "Word ID sequence: [549, 97, 554, 102, 173, 537, 325, 80, 296, 395, 183, 1, 305, 74, 37, 524, 593, 4, 1, 119, 271, 537, 161, 194, 582]\n",
      "\n",
      "\n",
      "Original: _book_title_ : thornton_waldo_burgess___the_advent\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' t', 'ho', 'rn', 'to', 'n_', 'wa', 'ld', 'o_', 'bu', 'rg', 'es', 's_', '__', 'th', 'e_', 'ad', 've', 'nt']\n",
      "Word ID sequence: [549, 97, 554, 102, 173, 537, 325, 3, 73, 212, 35, 619, 66, 99, 636, 139, 311, 61, 614, 524, 6, 537, 76, 48, 79]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, tokens, seq in zip(test_stories[:5], test_ngram_stories[:5], test_data_seq[:5]):\n",
    "    print(\"Original: {}\".format(s[:50]))\n",
    "    print(\"n-grams: {}\".format(tokens[:25]))\n",
    "    print(\"Word ID sequence: {}\".format(seq[:25]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the TensorFlow `tf.data` pipeline\n",
    "\n",
    "Here we will define a `tf.data` pipeline to generate data for the model. In language modelling, data is generated as follows. Say you want to provide a `n` elements long sequence as the input to the model in order to generate text. Then you take a `n+1` long sequence `text` and split it into two parts; `text[:-1]` and `text[1:]`. At any step of the implementation, you can check the specification of the dataset with `print(tf.data.DatasetSpec.from_value(ds))`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.1\n",
    "\n",
    "# Code listing 10.3\n",
    "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
    "    \"\"\" Define a tf.data pipeline that takes a set of sequences of text and \n",
    "    convert them to fixed length sequences for the model \"\"\"\n",
    "    \n",
    "    # Define a tf.dataset from a ragged tensor created from data_seq\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq)) # tf.ragged.constant(data_seq)\n",
    "    \n",
    "    # If shuffle is set, shuffle the data (shuffle story order)\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)\n",
    "    \n",
    "    # This function will create windows from data, given a window size and a shift\n",
    "    # Each window is a single entity    \n",
    "    \n",
    "    # windows function create neted dataset within text ds\n",
    "    # This is a special trick we use to unwrap those nested structures\n",
    "    #text_ds = text_ds.flat_map(lambda window: window.batch(n_seq+1, drop_remainder=True))    \n",
    "    text_ds = text_ds.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(\n",
    "            x\n",
    "        ).window(\n",
    "            n_seq+1, shift=shift\n",
    "        ).flat_map(\n",
    "            lambda window: window.batch(n_seq+1, drop_remainder=True)\n",
    "        )\n",
    "    ) \n",
    "    \n",
    "    # Shuffle the data (shuffle the order of n_seq+1 long sequences)\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
    "    \n",
    "    # Batch the data\n",
    "    text_ds = text_ds.batch(batch_size)\n",
    "    \n",
    "    # Split each sequence to an input and a target\n",
    "    text_ds = tf.data.Dataset.zip(text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return text_ds    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some data\n",
    "\n",
    "Here you can see that `a` is a tuple with two Tensors; an input tensor and a target tensor. If you check the target tensor, each row in target is essentially a shift by 1 to the right of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 15, 152,  85,  84,  30],\n",
      "       [325,  52, 229, 583, 105],\n",
      "       [173, 537, 325,  52, 229],\n",
      "       [ 87,   6,   2,  72,  76],\n",
      "       [  8,  49, 103,  22,  31],\n",
      "       [ 31,   7,  22,  11, 279]])>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[152,  85,  84,  30, 349],\n",
      "       [ 52, 229, 583, 105, 296],\n",
      "       [537, 325,  52, 229, 583],\n",
      "       [  6,   2,  72,  76,  86],\n",
      "       [ 49, 103,  22,  31,   7],\n",
      "       [  7,  22,  11, 279,  64]])>)\n",
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 77,  23,  93, 206,  56],\n",
      "       [583, 105, 296, 641,  43],\n",
      "       [  2,  13, 123, 276, 486],\n",
      "       [  6, 537,  49, 112,  22],\n",
      "       [ 22,  31,   7,  22,  11],\n",
      "       [102, 173, 537, 325,  52]])>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 23,  93, 206,  56,  19],\n",
      "       [105, 296, 641,  43, 565],\n",
      "       [ 13, 123, 276, 486,  56],\n",
      "       [537,  49, 112,  22, 582],\n",
      "       [ 31,   7,  22,  11, 279],\n",
      "       [173, 537, 325,  52, 229]])>)\n"
     ]
    }
   ],
   "source": [
    "ds = get_tf_pipeline(train_data_seq, 5, batch_size=6)\n",
    "\n",
    "for a in ds.take(2):\n",
    "\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and save hyperparameters so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_grams uses n=2\n",
      "Vocabulary size: 741\n",
      "Sequence length for model: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"n_grams uses n={}\".format(ngrams))\n",
    "print(\"Vocabulary size: {}\".format(n_vocab))\n",
    "\n",
    "n_seq=100\n",
    "print(\"Sequence length for model: {}\".format(n_seq))\n",
    "\n",
    "with open(os.path.join('models', 'text_hyperparams.pkl'), 'wb') as f:\n",
    "    pickle.dump({'n_vocab': n_vocab, 'ngrams':ngrams, 'n_seq': n_seq}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "Here we're going to define an embedding layer, a single LSTM layer and two dense layers. \n",
    "\n",
    "More on regularizing LSTM models: https://arxiv.org/pdf/1708.02182.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.2\n",
    "\n",
    "# Code listing 10.4\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True),\n",
    "    \n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    \n",
    "    # Defining a final Dense layer and softmax activation\n",
    "    tf.keras.layers.Dense(n_vocab, name='final_out'),\n",
    "    tf.keras.layers.Activation(activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Perplexity Metric\n",
    "\n",
    "Perplexity measures given a sequence of $n-1$ words, how surprised (or perplexed) the model was to see the $n^{th}$ word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.3\n",
    "\n",
    "# Code listing 10.5\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Inspired by https://gist.github.com/Gregorgeous/dbad1ec22efc250c76354d949a13cec3\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    \n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super().__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "      # The next 4 lines zero-out the padding from loss calculations, \n",
    "      # this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t      \n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "      \n",
    "      # Calculating the perplexity steps: \n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      perplexity = K.exp(step1)\n",
    "      #perplexity = K.mean(step2)\n",
    "    \n",
    "      return perplexity \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):            \n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "      # Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n",
    "      super().update_state(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Perpelxity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.2082006, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p = PerplexityMetric()\n",
    "# Define a set of true targets\n",
    "true = [[0, 1,2],[0, 1,2]]\n",
    "# Define a set of predictions\n",
    "pred = [[[0.9, 0.1, 0.0], [0.3, 0.7, 0.0], [0.0, 0.1, 0.9]],[[0.9, 0.1, 0.0], [0.3, 0.7, 0.0], [0.0, 0.1, 0.9]]]\n",
    "\n",
    "# Compute perplexity\n",
    "p.update_state(true, pred)\n",
    "print(p.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "We will compile the model with `sparse_categorical_crossentropy`, `adam` optimizer and `accuracy` and `perplexity` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 512)         379904    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 1024)        4724736   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 512)         524800    \n",
      "                                                                 \n",
      " final_out (Dense)           (None, None, 741)         380133    \n",
      "                                                                 \n",
      " activation (Activation)     (None, None, 741)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,009,573\n",
      "Trainable params: 6,009,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Here we're going to train the model. To keep the training shorter, we will only use 50/98 storings in the training set. We will also generate sequences at a shift of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using metric=val_perplexity and mode=min for EarlyStopping\n",
      "Epoch 1/50\n",
      "     68/Unknown - 14s 156ms/step - loss: 4.8875 - accuracy: 0.1212 - perplexity: 186.0962"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m es_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     34\u001b[0m     monitor\u001b[38;5;241m=\u001b[39mmonitor_metric, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, mode\u001b[38;5;241m=\u001b[39mmode, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t2\u001b[38;5;241m-\u001b[39mt1))\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section 10.4\n",
    "\n",
    "train_ds = get_tf_pipeline(train_data_seq[:50], n_seq, shift=25, batch_size=128)\n",
    "valid_ds = get_tf_pipeline(val_data_seq, n_seq, shift=n_seq, batch_size=128)\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_language_modelling.log'))\n",
    "\n",
    "monitor_metric = 'val_perplexity'\n",
    "mode = 'min' \n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "# This function keeps the initial learning rate for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def scheduler(epoch, lr):  \n",
    "    if epoch==0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "#lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=2, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=5, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, epochs=50, \n",
    "          validation_data = valid_ds,\n",
    "          callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 5s 62ms/step - loss: 3.9177 - accuracy: 0.2052 - perplexity: 52.4445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.9176838397979736, 0.2052459865808487, 52.44453430175781]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "test_ds = get_tf_pipeline(test_data_seq, n_seq, shift=n_seq, batch_size=batch_size)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(model, os.path.join('models', '2_gram_lm.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'text_hyperparams.pkl'), 'rb') as f:\n",
    "    hparams = pickle.load(f)\n",
    "\n",
    "ngrams = hparams['ngrams']\n",
    "n_vocab = hparams[\"n_vocab\"]\n",
    "n_seq = hparams[\"n_seq\"]\n",
    "\n",
    "model = tf.keras.models.load_model(os.path.join('models', '2_gram_lm.h5'), compile=False)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inference model (Functional API)\n",
    "\n",
    "Here, we're going to define an inference model. We need to actually define a new model with identical weights to the original but will make changes to inputs and outputs. Essentially, we will define a model to which we can pass in an intial state (hidden state of GRU) and outputs the final prediction as well as the new hidden state.\n",
    "\n",
    "This way we can recursively call our model on new predictions to generate a story for any number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 512)    379904      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1024)]       0           []                               \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    [(None, None, 1024)  4724736     ['embedding_1[0][0]',            \n",
      "                                , (None, 1024)]                   'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 512)    524800      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " final_out (Dense)              (None, None, 741)    380133      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, None, 741)    0           ['final_out[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,009,573\n",
      "Trainable params: 6,009,573\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 10.5\n",
    "\n",
    "# Code listing 10.6\n",
    "\n",
    "# Define inputs to the model\n",
    "inp = tf.keras.layers.Input(shape=(None,))\n",
    "inp_state = tf.keras.layers.Input(shape=(1024,))\n",
    "\n",
    "# Define embedding layer and output\n",
    "emb_layer = tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,))\n",
    "emb_out = emb_layer(inp)\n",
    "\n",
    "# Defining a GRU layer and output\n",
    "gru_layer = tf.keras.layers.GRU(1024, return_state=True, return_sequences=True)\n",
    "gru_out, gru_state = gru_layer(emb_out, initial_state=inp_state)\n",
    "\n",
    "# Defining a Dense layer and output\n",
    "dense_layer = tf.keras.layers.Dense(512, activation='relu')\n",
    "dense_out = dense_layer(gru_out)\n",
    "\n",
    "# Defining the final Dense layer and output\n",
    "final_layer = tf.keras.layers.Dense(n_vocab, name='final_out')\n",
    "final_out = final_layer(dense_out)\n",
    "softmax_out = tf.keras.layers.Activation(activation='softmax')(final_out)\n",
    "\n",
    "# Define final model\n",
    "infer_model = tf.keras.models.Model(inputs=[inp, inp_state], outputs=[softmax_out, gru_state])\n",
    "\n",
    "# Copy the weights from the original model\n",
    "emb_layer.set_weights(model.get_layer('embedding').get_weights())\n",
    "gru_layer.set_weights(model.get_layer('gru').get_weights())\n",
    "dense_layer.set_weights(model.get_layer('dense').get_weights())\n",
    "final_layer.set_weights(model.get_layer('final_out').get_weights())\n",
    "\n",
    "# Summary\n",
    "infer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text with greedy decoding\n",
    "\n",
    "Here we will generate text with the simplest approach we can think of. At time $t=1$, we start with a predefined sequence, and feed that to `infer_model`. At the end of the sequence we get $w_1$ (the prediction at $t=1$). $w_1$ will be the input to the model at $t=2$ and the model will generate $w_2$ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions from a 54 element long input\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final text: \n",
      "chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank , and the wast the , anid , and the was and the wast ther , and the was and the wast the , and , and , any , and , and to the was the wast that and the wast the , and , and to the wast the was the , anid , and the wast the wang the wast the was that and the wast the , and , and , and , any , and to the wast the , any , and , and , and to the wast the , anid , and the wast that and the wast the , and , and , and , any .\n",
      " ` and the wast the , and to the wast ther , and the was the wast that and the wast the , anid , and the wast that and the wast the , anid , and the wast ther , and the wast that and the wast the , and , an the wast the , anid , and the wast the , and , and , and , any , and to the wast the , anid , and the wast the wang the wast ther , and the wast the was the , and , and , any , and the , and to the wast the wall the was the wast the wall the wast that and the wast the , anid , and the wast that and the wast that and the wast ther , and the wast the , any .\n",
      " ` and the was\n"
     ]
    }
   ],
   "source": [
    "# Section 10.5\n",
    "\n",
    "text = get_ngrams(\n",
    "    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank ,\".lower(), \n",
    "    ngrams\n",
    ")\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# build up model state using the given string\n",
    "print(\"Making predictions from a {} element long input\".format(len(seq[0])))\n",
    "\n",
    "# Reset the state of the model initially\n",
    "model.reset_states()\n",
    "# Definin the initial state as all zeros\n",
    "state = np.zeros(shape=(1,1024))\n",
    "# Recursively update the model by assining new state to state\n",
    "for c in seq[0]:    \n",
    "    out, state = infer_model.predict([np.array([[c]]), state], verbose=0)\n",
    "\n",
    "# Get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = tokenizer.index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "# Define first input to generate text recursively from\n",
    "x = np.array([[wid]])\n",
    "\n",
    "# Code listing 10.7\n",
    "for _ in range(500):\n",
    "    \n",
    "    # Get the next output and state\n",
    "    out, state = infer_model.predict([x, state], verbose=0)\n",
    "    \n",
    "    # Get the word id and the word from out\n",
    "    out_argsort = np.argsort(out[0], axis=-1).ravel()        \n",
    "    wid = int(out_argsort[-1])\n",
    "    word = tokenizer.index_word[wid]\n",
    "    \n",
    "    # If the word ends with space, we introduce a bit of randomness\n",
    "    # Essentially pick one of the top 3 outputs for that timestep depending on their likelihood\n",
    "    if word.endswith(' '):\n",
    "        if np.random.normal()>0.5:\n",
    "            width = 3\n",
    "            i = np.random.choice(list(range(-width,0)), p=out_argsort[-width:]/out_argsort[-width:].sum())    \n",
    "            wid = int(out_argsort[i])    \n",
    "            word = tokenizer.index_word[wid]\n",
    "            \n",
    "    # Append the prediction\n",
    "    text.append(word)\n",
    "    \n",
    "    # Recursively make the current prediction the next input\n",
    "    x = np.array([[wid]])\n",
    "    \n",
    "# Print the final output    \n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search decoding\n",
    "\n",
    "Beam search is a more sophisticated and better decoding technique. In beam search we predict several timesteps in to the future and pick the sequence that gives the best joint probability. Remember that, in greedy decoding we only predicted 1 step into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the beam search logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.6\n",
    "\n",
    "# Code listing 10.8\n",
    "\n",
    "def beam_one_step(model, input_, state):    \n",
    "    \"\"\" Perform the model update and output for one step\"\"\"\n",
    "    output, new_state = model.predict([input_, state], verbose=0)\n",
    "    return output, new_state\n",
    "\n",
    "\n",
    "def beam_search(model, input_, state, beam_depth=5, beam_width=3, ignore_blank=True):\n",
    "    \"\"\" Defines an outer wrapper for the computational function of beam search \"\"\"\n",
    "    \n",
    "    def recursive_fn(input_, state, sequence, log_prob, i):\n",
    "        \"\"\" This function performs actual recursive computation of the long string\"\"\"\n",
    "        \n",
    "        if i == beam_depth:\n",
    "            \"\"\" Base case: Terminate the beam search \"\"\"\n",
    "            results.append((list(sequence), state, np.exp(log_prob)))            \n",
    "            return sequence, log_prob, state\n",
    "        else:\n",
    "            \"\"\" Recursive case: Keep computing the output using the previous outputs\"\"\"\n",
    "            output, new_state = beam_one_step(model, input_, state)\n",
    "            \n",
    "            # Get the top beam_widht candidates for the given depth\n",
    "            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)\n",
    "            top_probs, top_ids = top_probs.numpy().ravel(), top_ids.numpy().ravel()\n",
    "            \n",
    "            # For each candidate compute the next prediction\n",
    "            for p, wid in zip(top_probs, top_ids):                \n",
    "                new_log_prob = log_prob + np.log(p)\n",
    "                \n",
    "                # we are going to penalize joint probability whenever the same symbol is repeating\n",
    "                if len(sequence)>0 and wid == sequence[-1]:\n",
    "                    new_log_prob = new_log_prob + np.log(1e-1)\n",
    "                    \n",
    "                sequence.append(wid)                \n",
    "                _ = recursive_fn(np.array([[wid]]), new_state, sequence, new_log_prob, i+1)                                         \n",
    "                sequence.pop()\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    sequence = []\n",
    "    log_prob = 0.0\n",
    "    recursive_fn(input_, state, sequence, log_prob, 0)    \n",
    "\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the actual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making 54 predictions from input\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Predict for 100 time steps\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):    \n\u001b[0;32m     29\u001b[0m     \n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Get the results from beam search\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Get one of the top 10 results based on their likelihood\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     n_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([p \u001b[38;5;28;01mfor\u001b[39;00m _,_,p \u001b[38;5;129;01min\u001b[39;00m result[:\u001b[38;5;241m10\u001b[39m]])\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mbeam_search\u001b[1;34m(model, input_, state, beam_depth, beam_width, ignore_blank)\u001b[0m\n\u001b[0;32m     43\u001b[0m sequence \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     44\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mrecursive_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     47\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m2\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mbeam_search.<locals>.recursive_fn\u001b[1;34m(input_, state, sequence, log_prob, i)\u001b[0m\n\u001b[0;32m     35\u001b[0m     new_log_prob \u001b[38;5;241m=\u001b[39m new_log_prob \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[0;32m     37\u001b[0m sequence\u001b[38;5;241m.\u001b[39mappend(wid)                \n\u001b[1;32m---> 38\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_log_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m                                         \n\u001b[0;32m     39\u001b[0m sequence\u001b[38;5;241m.\u001b[39mpop()\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mbeam_search.<locals>.recursive_fn\u001b[1;34m(input_, state, sequence, log_prob, i)\u001b[0m\n\u001b[0;32m     35\u001b[0m     new_log_prob \u001b[38;5;241m=\u001b[39m new_log_prob \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[0;32m     37\u001b[0m sequence\u001b[38;5;241m.\u001b[39mappend(wid)                \n\u001b[1;32m---> 38\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_log_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m                                         \n\u001b[0;32m     39\u001b[0m sequence\u001b[38;5;241m.\u001b[39mpop()\n",
      "    \u001b[1;31m[... skipping similar frames: beam_search.<locals>.recursive_fn at line 38 (3 times)]\u001b[0m\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mbeam_search.<locals>.recursive_fn\u001b[1;34m(input_, state, sequence, log_prob, i)\u001b[0m\n\u001b[0;32m     35\u001b[0m     new_log_prob \u001b[38;5;241m=\u001b[39m new_log_prob \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[0;32m     37\u001b[0m sequence\u001b[38;5;241m.\u001b[39mappend(wid)                \n\u001b[1;32m---> 38\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_log_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m                                         \n\u001b[0;32m     39\u001b[0m sequence\u001b[38;5;241m.\u001b[39mpop()\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mbeam_search.<locals>.recursive_fn\u001b[1;34m(input_, state, sequence, log_prob, i)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124;03m\"\"\" Recursive case: Keep computing the output using the previous outputs\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     output, new_state \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Get the top beam_widht candidates for the given depth\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     top_probs, top_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mtop_k(output, k\u001b[38;5;241m=\u001b[39mbeam_width)\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mbeam_one_step\u001b[1;34m(model, input_, state)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbeam_one_step\u001b[39m(model, input_, state):    \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124;03m\"\"\" Perform the model update and output for one step\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     output, new_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, new_state\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py:2029\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2027\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[0;32m   2028\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2029\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2030\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2031\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\data_adapter.py:1193\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1193\u001b[0m   data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:494\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    493\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    497\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    717\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    718\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v3(\n\u001b[0;32m    719\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[0;32m    720\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 721\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3407\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3408\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3409\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3411\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section 10.6\n",
    "\n",
    "# Code listing 10.9\n",
    "\n",
    "text = get_ngrams(\n",
    "    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank ,\".lower(),     \n",
    "    ngrams\n",
    ")\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# build up model state using the given string\n",
    "print(\"Making {} predictions from input\".format(len(seq[0])))\n",
    "\n",
    "#model.reset_states()\n",
    "state = np.zeros(shape=(1,1024))\n",
    "for c in seq[0]:    \n",
    "    out, state = infer_model.predict([np.array([[c]]), state], verbose=0)\n",
    "\n",
    "# get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = tokenizer.index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "x = np.array([[wid]])\n",
    "\n",
    "# Predict for 100 time steps\n",
    "for i in range(100):    \n",
    "    \n",
    "    # Get the results from beam search\n",
    "    result = beam_search(infer_model, x, state, 7, 2)\n",
    "    \n",
    "    # Get one of the top 10 results based on their likelihood\n",
    "    n_probs = np.array([p for _,_,p in result[:10]])\n",
    "    p_j = np.random.choice(list(range(n_probs.size)), p=n_probs/n_probs.sum())                    \n",
    "    best_beam_ids, state, _ = result[p_j]\n",
    "    x = np.array([[best_beam_ids[-1]]])\n",
    "            \n",
    "    text.extend([tokenizer.index_word[w] for w in best_beam_ids])    \n",
    "\n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
