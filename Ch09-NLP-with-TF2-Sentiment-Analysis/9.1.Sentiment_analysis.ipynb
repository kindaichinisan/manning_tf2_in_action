{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (Natural Language Processing)\n",
    "\n",
    "Natural Language Processing (NLP) is a vast subject with many different specializations. Here we are going to discuss sentiment analysis. We will be specifically looking at developing a sentiment model based on Amazon video game reviews.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch09-NLP-with-TF2-Sentiment-Analysis/9.1.Sentiment_analysis.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and other housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import requests\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "\n",
    "Here as the dataset we are going to use an Amazon video game review dataset available through this [link](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Section 9.1\n",
    "\n",
    "# Code listing 9.1\n",
    "\n",
    "# Downloading the data\n",
    "# http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','Video_Games_5.json.gz')):\n",
    "    url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data','Video_Games_5.json.gz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'Video_Games_5.json')):\n",
    "    with gzip.open(os.path.join('data','Video_Games_5.json.gz'), 'rb') as f_in:\n",
    "        with open(os.path.join('data','Video_Games_5.json'), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading review data\n",
    "\n",
    "Let's load a sample of data. The columns that are of interest to us are,\n",
    "\n",
    "* overall - The overall stars received in the reivew\n",
    "* verified - Whether the reviewer is a verified buyer or not\n",
    "* reviewTime - When the review was posted\n",
    "* reviewText - The review itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 17, 2015</td>\n",
       "      <td>This game is a bit hard to get the hang of, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>07 27, 2015</td>\n",
       "      <td>I played it a while but it was alright. The st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2015</td>\n",
       "      <td>ok game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>02 20, 2015</td>\n",
       "      <td>found the game a bit too complicated, not what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>12 25, 2014</td>\n",
       "      <td>great game, I love it and have played it since...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime  \\\n",
       "0        5      True  10 17, 2015   \n",
       "1        4     False  07 27, 2015   \n",
       "2        3      True  02 23, 2015   \n",
       "3        2      True  02 20, 2015   \n",
       "4        5      True  12 25, 2014   \n",
       "\n",
       "                                          reviewText  \n",
       "0  This game is a bit hard to get the hang of, bu...  \n",
       "1  I played it a while but it was alright. The st...  \n",
       "2                                           ok game.  \n",
       "3  found the game a bit too complicated, not what...  \n",
       "4  great game, I love it and have played it since...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file\n",
    "review_df = pd.read_json(os.path.join('data', 'Video_Games_5.json'), lines=True, orient='records')\n",
    "# Select on the columns we're interested in \n",
    "review_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up data\n",
    "\n",
    "Remove entries where the description is null or empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning up: (497577, 4)\n",
      "After cleaning up: (497419, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning up: {}\".format(review_df.shape))\n",
    "review_df = review_df[~review_df[\"reviewText\"].isna()]\n",
    "review_df = review_df[review_df[\"reviewText\"].str.strip().str.len()>0]\n",
    "print(\"After cleaning up: {}\".format(review_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking verified vs non-verified review counts\n",
    "\n",
    "To improve the quality of the data reviewed, we will only use the verified reviews. But we have to make sure there's enough data. It seems more than 66% of the data is coming from verified buyers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     332504\n",
       "False    164915\n",
       "Name: verified, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df[\"verified\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the review count for each rating from verified buyers\n",
    "\n",
    "Let's check how many reviews are there for each different rating. As you can see there are way too many 5 star reviews in our dataset. Therefore we must make sure to account for this imbalance when batching data and creating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    222335\n",
       "4     54878\n",
       "3     27973\n",
       "1     15200\n",
       "2     12118\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_df = review_df.loc[review_df[\"verified\"], :]\n",
    "verified_df[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map rating to a positive/negative label\n",
    "\n",
    "To make our classification task simple, we will map the different star ratings to a positive (1) or a negative (0) label. Here we map both 5 and 4 reviews to 1 (positive) and 3,2, and 1 to 0 (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thush\\AppData\\Local\\Temp\\ipykernel_29612\\2526005443.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  verified_df[\"label\"]=verified_df[\"overall\"].map({5:1, 4:1, 3:0, 2:0, 1:0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    277213\n",
       "0     55291\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas map function to map different star ratings to 0/1\n",
    "verified_df[\"label\"]=verified_df[\"overall\"].map({5:1, 4:1, 3:0, 2:0, 1:0})\n",
    "verified_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the data\n",
    "\n",
    "Let's shuffle the data to make sure there is no order in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are sampling 100% of the data in a random fashion, leading to a shuffled dataset\n",
    "verified_df = verified_df.sample(frac=1.0, random_state=random_seed)\n",
    "\n",
    "# Splint the data to inputs (inputs) and targets (labels)\n",
    "inputs, labels = verified_df[\"reviewText\"], verified_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text\n",
    "\n",
    "Here we perform preprocessing. Mainly we're going to focus on the following \n",
    "\n",
    "* Lower case (nltk) - Turn \"I am\" to \"i am\"\n",
    "* Remove numbers (regex) - Turn \"i am 24 years old\" to \"i am years old\"\n",
    "* Remove stop words (nltk) - Turn \"i go to the shop\" to \"i go shop\"\n",
    "* Lemmatize (nltk) - Turn \"i went to buy flowers\" to \"i go to buy flower\"\n",
    "\n",
    "Preprocessing helps to reduce the features space, thus the model learning faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to nltk...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before clean: She sells seashells by the seashore.\n",
      "After clean: ['sell', 'seashell', 'seashore']\n",
      "Data already found. If you want to rerun anyway, set rerun=True\n"
     ]
    }
   ],
   "source": [
    "# Section 9.1\n",
    "\n",
    "import nltk\n",
    "# We need to download several nltk artefacts to perform the preprocessing\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
    "nltk.download('wordnet', download_dir='nltk')\n",
    "nltk.download('stopwords', download_dir='nltk')\n",
    "nltk.download('punkt', download_dir='nltk')\n",
    "nltk.data.path.append(os.path.abspath('nltk'))\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "rerun = False\n",
    "\n",
    "# Define a lemmatizer (converts words to base form)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the English stopwords\n",
    "EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n",
    "\n",
    "# Code listing 9.2\n",
    "def clean_text(doc):\n",
    "    \"\"\" A function that cleans a given document (i.e. a text string)\"\"\"\n",
    "    \n",
    "    # Turn to lower case\n",
    "    doc = doc.lower()\n",
    "    # the shortened form n't is expanded to not\n",
    "    doc = re.sub(pattern=r\"\\w+n\\'t \", repl=\"not \", string=doc)\n",
    "    # shortened forms like 'll 're 'd 've are removed as they don't add much value to this task\n",
    "    doc = re.sub(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \", doc)\n",
    "    # numbers are removed\n",
    "    doc = re.sub(r\"/d+\",\"\", doc)\n",
    "    # break the text in to tokens (or words), while doing that ignore stopwords from the result\n",
    "    # stopwords again do not add any value to the task\n",
    "    tokens = [w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in string.punctuation]  \n",
    "    \n",
    "    # Here we lemmatize the words in the tokens\n",
    "    # to lemmatize, we get the pos tag of each token and \n",
    "    # if it is N (noun) or V (verb) we lemmatize, else \n",
    "    # keep the original form\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    clean_text = [\n",
    "        lemmatizer.lemmatize(w, pos=p[0].lower()) \\\n",
    "        if p[0]=='N' or p[0]=='V' else w \\\n",
    "        for (w, p) in pos_tags\n",
    "    ]\n",
    "\n",
    "    # return the clean text\n",
    "    return clean_text\n",
    "\n",
    "# Run a sample\n",
    "sample_doc = 'She sells seashells by the seashore.'\n",
    "print(\"Before clean: {}\".format(sample_doc))\n",
    "print(\"After clean: {}\".format(clean_text(sample_doc)))\n",
    "\n",
    "if rerun or \\\n",
    "    not os.path.exists(os.path.join('data','sentiment_inputs.pkl')) or \\\n",
    "    not os.path.exists(os.path.join('data','sentiment_labels.pkl')):\n",
    "    # Apply the transformation to the full text\n",
    "    # this is time consuming\n",
    "    print(\"\\nProcessing all the review data ... This can take a long time (~ 1hr)\")\n",
    "    inputs = inputs.apply(lambda x: clean_text(x))\n",
    "    print(\"\\tDone\")\n",
    "    \n",
    "    print(\"Saving the data\")\n",
    "    inputs.to_pickle(os.path.join('data','sentiment_inputs.pkl'))\n",
    "    labels.to_pickle(os.path.join('data','sentiment_labels.pkl'))\n",
    "    \n",
    "else:\n",
    "    # Load the data from the disk\n",
    "    print(\"Data already found. If you want to rerun anyway, set rerun=True\")\n",
    "    inputs = pd.read_pickle(os.path.join('data', 'sentiment_inputs.pkl'))\n",
    "    labels = pd.read_pickle(os.path.join('data', 'sentiment_labels.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how preprocessing has changed the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Worked perfectly on Wii and Gamecube.\n",
      "No issues with compatibility or loss of memory.\n",
      "Clean: ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', 'loss', 'memory']\n",
      "\n",
      "\n",
      "Actual: Loved the game and the other collectibles that came with it are well made.  The mask is big and it almost fits my face so that was impressive.\n",
      "Clean: ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 'big', 'almost', 'fit', 'face', 'impressive']\n",
      "\n",
      "\n",
      "Actual: It's an okay game, to be honest, I am very bad at these types of games and to me-- it's very difficult! I am always dying, which depresses me. Maybe if I had more skill I would enjoy this game more!\n",
      "Clean: [\"'s\", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', \"'s\", 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 'enjoy', 'game']\n",
      "\n",
      "\n",
      "Actual: Excellent product as described\n",
      "Clean: ['excellent', 'product', 'describe']\n",
      "\n",
      "\n",
      "Actual: the level of detail is great you can feel the love for cars in this game.\n",
      "Clean: ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for actual, clean in zip(verified_df[\"reviewText\"].iloc[:5], inputs.iloc[:5]):\n",
    "    print(\"Actual: {}\".format(actual))\n",
    "    print(\"Clean: {}\".format(clean))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some reviews and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122143    [work, perfectly, wii, gamecube, issue, compat...\n",
      "444818    [loved, game, collectible, come, well, make, m...\n",
      "79331     ['s, okay, game, honest, bad, type, game, --, ...\n",
      "97250                        [excellent, product, describe]\n",
      "324411        [level, detail, great, feel, love, car, game]\n",
      "427782       [buy, game, buy, vita, buy, game, 's, amazing]\n",
      "200896    [excellent, product, useful, .., 100, recommen...\n",
      "394648    [great, amiibos, add, collection, level, chall...\n",
      "480002    [first, zenses, game, buy, get, two, fast, cou...\n",
      "194630                                         [nice, game]\n",
      "122215    [awesome, memory, card, work, great, wii, 's, ...\n",
      "278172    [start, play, believe, great, game, take, get,...\n",
      "233269    [game, cool, alot, like, remember, way, everyh...\n",
      "299538    [might, seem, interesting, buy, friend, neurol...\n",
      "229458    [short, movement, issue, not, bad, could, 3, f...\n",
      "34481     [not, actually, believe, write, review, produc...\n",
      "258474    [good, game, us, like, movie, franchise, hard,...\n",
      "466203    [fun, first, person, shooter, nice, combinatio...\n",
      "414288                       [love, amiibo, classic, color]\n",
      "162670    [fan, halo, series, start, enjoy, game, overal...\n",
      "Name: reviewText, dtype: object\n",
      "122143    1\n",
      "444818    1\n",
      "79331     0\n",
      "97250     1\n",
      "324411    1\n",
      "427782    1\n",
      "200896    1\n",
      "394648    1\n",
      "480002    1\n",
      "194630    1\n",
      "122215    1\n",
      "278172    1\n",
      "233269    0\n",
      "299538    1\n",
      "229458    0\n",
      "34481     1\n",
      "258474    1\n",
      "466203    1\n",
      "414288    1\n",
      "162670    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(inputs.head(n=20))\n",
    "print(labels.head(n=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data to train/valid/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 310388\n",
      "Validation data: 11058\n",
      "Test data: 11058\n"
     ]
    }
   ],
   "source": [
    "# Section 9.2\n",
    "\n",
    "# Code listing 9.3\n",
    "def train_valid_test_split(inputs, labels, train_fraction=0.8):\n",
    "    \"\"\" Splits a given dataset into three sets; training, validation and test \"\"\"    \n",
    "    \n",
    "    # Separate indices of negative and positive data points\n",
    "    neg_indices = pd.Series(labels.loc[(labels==0)].index)\n",
    "    pos_indices = pd.Series(labels.loc[(labels==1)].index)\n",
    "    \n",
    "    n_valid = int(min([len(neg_indices), len(pos_indices)]) * ((1-train_fraction)/2.0))\n",
    "    n_test = n_valid\n",
    "    \n",
    "    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\n",
    "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())]\n",
    "    \n",
    "    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)\n",
    "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    pos_train_inds = pos_indices.loc[\n",
    "        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())\n",
    "    ]\n",
    "    \n",
    "    tr_x = inputs.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    tr_y = labels.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_x = inputs.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_y = labels.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_x = inputs.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_y = labels.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    \n",
    "    print('Training data: {}'.format(len(tr_x)))\n",
    "    print('Validation data: {}'.format(len(v_x)))\n",
    "    print('Test data: {}'.format(len(ts_x)))\n",
    "    \n",
    "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
    "    \n",
    "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample targets\n",
      "409082    1\n",
      "402599    1\n",
      "247944    1\n",
      "276901    0\n",
      "200286    1\n",
      "278318    0\n",
      "483455    0\n",
      "486008    0\n",
      "405605    0\n",
      "491763    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Some sample targets\")\n",
    "print(tr_y.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the data (training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the vocabulary\n",
    "\n",
    "Let's see what are the most popular words as well as some summary statistics about the data (e.g. mean frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game     408819\n",
      "not      248729\n",
      "play     128822\n",
      "'s       128200\n",
      "get      109001\n",
      "like     100426\n",
      "great     97007\n",
      "one       90214\n",
      "good      77356\n",
      "time      63470\n",
      "dtype: int64\n",
      "count    133714.000000\n",
      "mean         75.768207\n",
      "std        1754.508881\n",
      "min           1.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           4.000000\n",
      "max      408819.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Section 9.2\n",
    "\n",
    "from collections import Counter\n",
    "# Create a large list which contains all the words in all the reviews\n",
    "data_list = [w for doc in tr_x for w in doc]\n",
    "\n",
    "# Create a Counter object from that list\n",
    "# Counter returns a dictionary, where key is a word and the value is the frequency\n",
    "cnt = Counter(data_list)\n",
    "\n",
    "# Convert the result to a pd.Series \n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "# Print most common words\n",
    "print(freq_df.head(n=10))\n",
    "\n",
    "# Print summary statistics\n",
    "print(freq_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the sequence length (number of words) of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some summary statistics\n",
      "Median length: 12.0\n",
      "\n",
      "\n",
      "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    278675.000000\n",
       "mean         15.422596\n",
       "std          16.258732\n",
       "min           1.000000\n",
       "33%           5.000000\n",
       "50%          10.000000\n",
       "66%          16.000000\n",
       "max          74.000000\n",
       "Name: reviewText, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pd.Series, which contain the sequence length for each review\n",
    "seq_length_ser = tr_x.str.len()\n",
    "\n",
    "# Get the median as well as summary statistics of the sequence length\n",
    "print(\"\\nSome summary statistics\")\n",
    "print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "seq_length_ser.describe()\n",
    "\n",
    "print(\"\\nComputing the statistics between the 10% and 90% quantiles (to ignore outliers)\")\n",
    "p_10 = seq_length_ser.quantile(0.1)\n",
    "p_90 = seq_length_ser.quantile(0.9)\n",
    "\n",
    "seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters\n",
    "\n",
    "Based on above analysis, define the vocabulary size and sequence lenght, both of which we need to define the data pipeline and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary of size: 11884\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df >= 25).sum()\n",
    "print(\"Using a vocabulary of size: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming text to numbers\n",
    "\n",
    "We will define a Keras tokenizer that will take sequences of words (tokens) and convert them to sequences of numbers. This is achieved by building a dictionary that maps a given word to an unique ID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Keras tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9.2\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer that will convert words to IDs\n",
    "# words that are less frequent will be replaced by 'unk'\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "\n",
    "# Fit the tokenizer on the data\n",
    "tokenizer.fit_on_texts(tr_x.tolist())\n",
    "\n",
    "# Convert all of train/validation/test data to sequences of IDs\n",
    "tr_x = tokenizer.texts_to_sequences(tr_x.tolist())\n",
    "v_x = tokenizer.texts_to_sequences(v_x.tolist())\n",
    "ts_x = tokenizer.texts_to_sequences(ts_x.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the outputs of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word id for \"game\" is: 2\n",
      "The word for id 4 is: play\n"
     ]
    }
   ],
   "source": [
    "# Checking the attributes of the tokenizer\n",
    "word = \"game\"\n",
    "wid = tokenizer.word_index[word]\n",
    "print(\"The word id for \\\"{}\\\" is: {}\".format(word, wid))\n",
    "wid = 4\n",
    "word = tokenizer.index_word[wid]\n",
    "print(\"The word for id {} is: {}\".format(wid, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do the reviews look like after converting words to word IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', 'loss', 'memory']\n",
      "Sequence: [14, 295, 83, 572, 121, 1974, 2223, 345]\n",
      "\n",
      "\n",
      "Text: ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 'big', 'almost', 'fit', 'face', 'impressive']\n",
      "Sequence: [1592, 2, 2031, 32, 23, 16, 2345, 153, 200, 155, 599, 1133]\n",
      "\n",
      "\n",
      "Text: [\"'s\", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', \"'s\", 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 'enjoy', 'game']\n",
      "Sequence: [5, 574, 2, 1264, 105, 197, 2, 112, 5, 274, 150, 354, 1, 290, 400, 19, 67, 2]\n",
      "\n",
      "\n",
      "Text: ['excellent', 'product', 'describe']\n",
      "Sequence: [109, 55, 501]\n",
      "\n",
      "\n",
      "Text: ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']\n",
      "Sequence: [60, 419, 8, 42, 13, 265, 2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert words to IDs\n",
    "test_text = [\n",
    "    ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', 'loss', 'memory'],\n",
    "    ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 'big', 'almost', 'fit', 'face', 'impressive'],\n",
    "    [\"'s\", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', \"'s\", 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 'enjoy', 'game'],\n",
    "    ['excellent', 'product', 'describe'],\n",
    "    ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "for text, seq in zip(test_text, test_seq):\n",
    "    print(\"Text: {}\".format(text))\n",
    "    print(\"Sequence: {}\".format(seq))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Primer on `tf.RaggedTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1, 2, 3], [], [4, 5, 6], [7]]>\n",
      "(4, None)\n",
      "<tf.RaggedTensor [[[1, 2, 3, 4], [5, 6]], [[7, 8, 9]]]>\n",
      "(2, None, None)\n",
      "<tf.RaggedTensor [[[1, 2, 3, 4], [5, 6]]]>\n",
      "<tf.RaggedTensor [[[1, 2, 3, 4]],\n",
      " [[7, 8, 9]]]>\n",
      "<tf.RaggedTensor [[[1, 2],\n",
      "  [5, 6]], [[7, 8]]]>\n"
     ]
    }
   ],
   "source": [
    "a = tf.ragged.constant([[1, 2, 3], [1,2], [1]])\n",
    "\n",
    "b = tf.RaggedTensor.from_row_splits([1,2,3,4,5,6,7], row_splits=[0, 3, 3, 6, 7])\n",
    "print(b)\n",
    "print(b.shape)\n",
    "\n",
    "c = tf.RaggedTensor.from_nested_row_splits(\n",
    "    flat_values=[1,2,3,4,5,6,7,8,9], \n",
    "    nested_row_splits=([0,2,3],[0,4,6,9]))\n",
    "print(c)\n",
    "print(c.shape)\n",
    "\n",
    "print(c[:1, :, :])\n",
    "print(c[:,:1,:])\n",
    "print(c[:, :, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the `tf.data` Pipeline\n",
    "\n",
    "Here we will define a `tf.data` pipeline that takes in,\n",
    "* A list of list, where the outer list contains the individual reviews and the inner list contains the word IDs of a review\n",
    "\n",
    "and perform,\n",
    "\n",
    "* Bucketing, to separate sequences with different lengths to predefined buckets (each bucket has predefined boundaries) and return batches containing seuqences of constant length (with the help of padding)\n",
    "* Shuffle the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9.3\n",
    "\n",
    "# Code listing 9.4\n",
    "def get_tf_pipeline(text_seq, labels, batch_size=64, bucket_boundaries=[5,15], max_length=50, shuffle=False):\n",
    "    \"\"\" Define a data pipeline that converts sequences to batches of data \"\"\"\n",
    "    \n",
    "    # Concatenate the label and the input sequence so that we don't mess up the order when we shuffle\n",
    "    data_seq = [[b]+a for a,b in zip(text_seq, labels) ]\n",
    "    # Define the variable sequence dataset as a ragged tensor\n",
    "    tf_data = tf.ragged.constant(data_seq)[:,:max_length]\n",
    "    # Create a dataset out of the ragged tensor\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)\n",
    "    \n",
    "    text_ds = text_ds.filter(lambda x: tf.size(x)>1)\n",
    "    # Bucketing the data\n",
    "    # Bucketing assign each sequence to a bucket depending on the length\n",
    "    # If you define bucket boundaries as [5, 15], then you get buckets,\n",
    "    # [0, 5], [5, 15], [15,inf]\n",
    "    bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
    "        lambda x: tf.cast(tf.shape(x)[0],'int32'), \n",
    "        bucket_boundaries=bucket_boundaries, \n",
    "        bucket_batch_sizes=[batch_size,batch_size,batch_size], \n",
    "        padded_shapes=None,\n",
    "        padding_values=0, \n",
    "        pad_to_bucket_boundary=False\n",
    "    )\n",
    "\n",
    "    # Apply bucketing\n",
    "    text_ds = text_ds.map(lambda x: x).apply(bucket_fn)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
    "        \n",
    "    # Split the data to inputs and labels\n",
    "    text_ds = text_ds.map(lambda x: (x[:,1:], x[:,0]))    \n",
    "    \n",
    "    return text_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the behavior of bucketing function\n",
    "\n",
    "Here we will look at what actually takes place when you perform bucketing on some sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thush\\AppData\\Local\\Temp\\ipykernel_29612\\2412154328.py:19: bucket_by_sequence_length (from tensorflow.python.data.experimental.ops.grouping) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.bucket_by_sequence_length(...)`.\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[1 2 0]\n",
      " [1 2 3]], shape=(2, 3), dtype=int32)\n",
      "\ty= tf.Tensor([0 0], shape=(2,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[   3    2    6  543    2 3243    2  134   52   23    0]\n",
      " [   3   32   21    3    2    4  134   45    1    1   45]], shape=(2, 11), dtype=int32)\n",
      "\ty= tf.Tensor([0 0], shape=(2,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "\ty= tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[  2   4 214  21   0   0   0   0]\n",
      " [  3   4  42   7   3   2  45  52]], shape=(2, 8), dtype=int32)\n",
      "\ty= tf.Tensor([1 0], shape=(2,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[2 3 6 4 5]\n",
      " [2 0 9 7 0]], shape=(2, 5), dtype=int32)\n",
      "\ty= tf.Tensor([1 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = [[1,2],[1],[1,2,3], [2,3,6,4,5],[2,0,9,7],[2,4,214,21],[3,4,42,7,3,2,45,52],[3,2,6,543,2,3243,2,134,52,23],[3,32,21,3,2,4,134,45,1,1,45]]\n",
    "y = [0,0,0, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "a = get_tf_pipeline(x, y, batch_size=2, bucket_boundaries=[3,5], max_length=15, shuffle=True)\n",
    "\n",
    "for x,y in a.take(6):\n",
    "    print('\\n')\n",
    "    print(x)\n",
    "    print('\\ty=', y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take out some example to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some training data ...\n",
      "Input sequence shape: (64, 49)\n",
      "tf.Tensor(\n",
      "[0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1], shape=(64,), dtype=int32)\n",
      "Input sequence shape: (64, 13)\n",
      "tf.Tensor(\n",
      "[1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(64,), dtype=int32)\n",
      "\n",
      "Some validation data ...\n",
      "Input sequence shape: (64, 49)\n",
      "tf.Tensor(\n",
      "[0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0\n",
      " 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0], shape=(64,), dtype=int32)\n",
      "Input sequence shape: (64, 49)\n",
      "tf.Tensor(\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
      " 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1], shape=(64,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_tf_pipeline(tr_x, tr_y, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y)\n",
    "\n",
    "print(\"Some training data ...\")\n",
    "for x,y in train_ds.take(2):\n",
    "    print(\"Input sequence shape: {}\".format(x.shape))\n",
    "    print(y)\n",
    "\n",
    "print(\"\\nSome validation data ...\")\n",
    "for x,y in valid_ds.take(2):\n",
    "    print(\"Input sequence shape: {}\".format(x.shape))\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the sentiment analysis model\n",
    "\n",
    "Here we are going to define a model. Our model has,\n",
    "\n",
    "* A masking layer, we will mask-out value zero inputs, which will not contribute to the loss function\n",
    "* A lambda layer that converts IDs to one hot vectors\n",
    "* A LSTM model with 128 nodes\n",
    "* A Dense layer that has 512 nodes and ReLU activation\n",
    "* Final Dense layer that outputs the sentiment (sigmoid activation)\n",
    "\n",
    "Finally, the model will have,\n",
    "* A binary crossentropy loss\n",
    "* Adam optimizer\n",
    "* Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, None, 1)           0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 1)           0         \n",
      "                                                                 \n",
      " onehot_encoder (OnehotEncod  (None, None, 11884)      0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               6150656   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,217,217\n",
      "Trainable params: 6,217,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 9.4\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "class OnehotEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OnehotEncoder, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        \n",
    "        inputs = tf.cast(inputs, 'int32')\n",
    "        \n",
    "        if len(inputs.shape) == 3:\n",
    "            inputs = inputs[:,:,0]\n",
    " \n",
    "        return tf.one_hot(inputs, depth=self.depth)\n",
    "\n",
    "            \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'depth': self.depth})\n",
    "        return config\n",
    "    \n",
    "# You will see the following error if you don't filter out all zero (empty) records from the dataset\n",
    "# these records return a vector of all zeros which leads the LSTM layer to error out\n",
    "# CUDNN_STATUS_BAD_PARAM\n",
    "# in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496): \n",
    "# 'cudnnSetRNNDataDescriptor( \n",
    "#     data_desc.get(), data_type, \n",
    "#     layout, \n",
    "#     max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill\n",
    "# )'\n",
    "\n",
    "# Code listing 9.5\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=(None,)),\n",
    "    # Create a mask to mask out zero inputs\n",
    "    tf.keras.layers.Masking(mask_value=0),\n",
    "    # After creating the mask, convert inputs to onehot encoded inputs\n",
    "    OnehotEncoder(depth=n_vocab),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking layer's mask\n",
      "tf.Tensor(\n",
      "[[ True  True  True False False]\n",
      " [ True  True  True  True False]], shape=(2, 5), dtype=bool)\n",
      "Onehot encoder layer's mask\n",
      "tf.Tensor(\n",
      "[[ True  True  True False False]\n",
      " [ True  True  True  True False]], shape=(2, 5), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "inp = tf.expand_dims(tf.constant([[2,3,4,0,0], [2,4,6,12,0]], dtype='int32'),axis=-1)\n",
    "# Create a mask to mask out zero inputs\n",
    "mask_out = tf.keras.layers.Masking(mask_value=0)(inp)\n",
    "print(\"Masking layer's mask\")\n",
    "print(mask_out._keras_mask)\n",
    "# After creating the mask, convert inputs to onehot encoded inputs\n",
    "onehot_out = OnehotEncoder(depth=10)(mask_out)\n",
    "print(\"Onehot encoder layer's mask\")\n",
    "print(onehot_out._keras_mask)\n",
    "# Defining an LSTM layer\n",
    "lstm_out = tf.keras.layers.LSTM(24, return_state=False, return_sequences=False)(\n",
    "    onehot_out, mask=onehot_out._keras_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data pipelines\n",
      "\tDone...\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining data pipelines\")\n",
    "\n",
    "# Using a batch size of 128\n",
    "batch_size =8 #128\n",
    "\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size)\n",
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)\n",
    "print('\\tDone...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the negative weights\n",
    "\n",
    "We discussed first that our dataset has a high class imbalance. Particularly, there are more positive inputs than negative ones. This means we need to weigh our negative examples more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be using a weight of 6.017113919471887 for negative samples\n"
     ]
    }
   ],
   "source": [
    "# There is a class imbalance in the data therefore we are defining a weight for negative inputs\n",
    "neg_weight = (tr_y==1).sum()/(tr_y==0).sum()\n",
    "print(\"Will be using a weight of {} for negative samples\".format(neg_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using metric=val_loss and mode=min for EarlyStopping\n",
      "Epoch 1/10\n",
      "   1329/Unknown - 17s 10ms/step - loss: 0.9606 - accuracy: 0.6904"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mneg_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t2\u001b[38;5;241m-\u001b[39mt1))\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section 9.5\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_sentiment_analysis.log'))\n",
    "\n",
    "monitor_metric = 'val_loss'\n",
    "mode = 'min'\n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight={0:neg_weight, 1:1.0}, callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(model, os.path.join('models', '1_sentiment_analysis.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1381/1381 [==============================] - 7s 5ms/step - loss: 0.4983 - accuracy: 0.7750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4982852339744568, 0.7749705910682678]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with an Embedding layer\n",
    "\n",
    "Next we are going to enhance our model using an Embedding layer. An embedding layer is very useful to capture relationships between different words. For example, in the context of sentiment analysis, words like \"good\", \"great\" should produce similar feature vectors. An embedding layer achieves this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         1521280   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,719,425\n",
      "Trainable params: 1,719,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 9.6\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Code listing 9.7\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    # Adding an Embedding layer    \n",
    "    # You will see the following error if you don't filter out all zero (empty) records from the dataset\n",
    "    # these records return a vector of all zeros which leads the LSTM layer to error out\n",
    "    # CUDNN_STATUS_BAD_PARAM\n",
    "    # in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496): \n",
    "    # 'cudnnSetRNNDataDescriptor( \n",
    "    #     data_desc.get(), data_type, \n",
    "    #     layout, \n",
    "    #     max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill\n",
    "    # )'\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, mask_zero=True, input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    # Defining Dense layers\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Defining a dropout layer\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the newly defined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data pipelines\n",
      "\tDone...\n",
      "Using metric=val_loss and mode=min for EarlyStopping\n",
      "Epoch 1/10\n",
      "   2321/Unknown - 19s 7ms/step - loss: 0.9239 - accuracy: 0.7138"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m es_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     27\u001b[0m     monitor\u001b[38;5;241m=\u001b[39mmonitor_metric, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, mode\u001b[38;5;241m=\u001b[39mmode, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mneg_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t2\u001b[38;5;241m-\u001b[39mt1))\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section 9.7\n",
    "\n",
    "print(\"Defining data pipelines\")\n",
    "batch_size=8 #128\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size,)\n",
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)\n",
    "print('\\tDone...')\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','2_sentiment_analysis_embeddings.log'))\n",
    "\n",
    "monitor_metric = 'val_loss'\n",
    "mode = 'min' if 'loss' in monitor_metric else 'max'\n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight={0:neg_weight, 1:1.0}, callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(model, os.path.join('models', '2_sentiment_analysis_embeddings.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test set\n",
    "\n",
    "Our new model gives slightly better accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 7ms/step - loss: 0.4759 - accuracy: 0.7817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47588568925857544, 0.7816716432571411]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse some of the results\n",
    "\n",
    "Here, let's analyse some of the inputs with strong positive/negative sentiments predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 5ms/step\n",
      "4/4 [==============================] - 2s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 675us/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 1s 0s/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "X: 11043\n",
      "Pred: (11043, 1)\n",
      "Y: (11043,)\n"
     ]
    }
   ],
   "source": [
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
    "\n",
    "# Go through the test data and gather all examples\n",
    "test_x = []\n",
    "test_pred = []\n",
    "test_y = []\n",
    "for x, y in test_ds:\n",
    "    test_x.append(x)    \n",
    "    test_pred.append(model.predict(x))\n",
    "    test_y.append(y)\n",
    "\n",
    "# Check the sizes\n",
    "test_x = [doc for t in test_x for doc in t.numpy().tolist()]\n",
    "print(\"X: {}\".format(len(test_x)))\n",
    "test_pred = tf.concat(test_pred, axis=0).numpy()\n",
    "print(\"Pred: {}\".format(test_pred.shape))\n",
    "test_y = tf.concat(test_y, axis=0).numpy()\n",
    "print(\"Y: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative reviews\n",
      "\n",
      "==================================================\n",
      "game alot fun pitch system great throw meter nice challenge addition frame-rate terrible patch run good installed hard drive like unk need good slider set fun realistic default one horrible must say disappointed unk graphic not know graphic framerate get worse somehow happen not make frame-rate better not horrible \n",
      "\n",
      "one thing would suggest go get memory unit get one lot memory space soon realize little could hold game game info want save though suppose not game much save bunch music crap 's okay unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk \n",
      "\n",
      "chief complaint game get boring quickly level basically look differnt character level require strategy finish character weapon skill really feel like game rush door disney compete littlebigplanet not well think unk game wear little say 25 buck tho not total waste money pay 50 would return unk unk unk \n",
      "\n",
      "doesnt work tried device several different time not work not pay enough try return throw trash unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk \n",
      "\n",
      "speak someone unfortunately unk preorder mail review come ... not tempt buy nostalgia 's sake 's not badly review 's plain bad nothing taste plot attempt humor review focus poor taste unk joke unfortunate may cause reader think reviewer not like game aspect game bad gameplay bad nothing not \n",
      "\n",
      "\n",
      "Most positive reviews\n",
      "\n",
      "==================================================\n",
      "outstanding great seller highly recommend thank unk unk unk unk unk unk unk \n",
      "\n",
      "excellent add-on already fun kit enjoy play much highly recommend unk unk unk \n",
      "\n",
      "love rayman universe great music cool art fun gameplay heard origins good co-op unk chance play fun unk game girlfriend best fun ps3 highly recommend unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk \n",
      "\n",
      "muy buen producto entrega tiempo lo recomiendo calidad servicio good excelente unk unk \n",
      "\n",
      "christmas gift love long happy happy unk unk unk unk unk unk unk \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_pred = np.argsort(test_pred.flatten())\n",
    "min_pred = sorted_pred[:5]\n",
    "max_pred = sorted_pred[-5:]\n",
    "\n",
    "print(\"Most negative reviews\\n\")\n",
    "print(\"=\"*50)\n",
    "for i in min_pred:    \n",
    "    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')\n",
    "    \n",
    "print(\"\\nMost positive reviews\\n\")\n",
    "print(\"=\"*50)\n",
    "for i in max_pred:\n",
    "    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Training the model with a custom loss\n",
    "\n",
    "This code is a bonus section and an experimental one. Essentially, we are trying to see if adaptively using a negative weight on samples (depending on how much negatives/positive samples in a batch), can help the model to perform better. But it doesn't seem to have a strong effect since we're already weight the negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, None)              0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, None, 11884)       0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               12432384  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               25700     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,458,185\n",
      "Trainable params: 12,458,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Create a mask to mask out zero inputs\n",
    "    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,)),\n",
    "    # After creating the mask, convert inputs to onehot encoded inputs\n",
    "    tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x,'int32'), depth=n_vocab), input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(256, return_state=False, return_sequences=False),\n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    \n",
    "    pos_mask = tf.cast(tf.math.equal(y_true, 1),'float32')\n",
    "    n_pos = tf.reduce_sum(pos_mask)\n",
    "    neg_mask = tf.cast(tf.math.equal(y_true, 0),'float32')\n",
    "    n_neg = tf.reduce_sum(neg_mask)\n",
    "    \n",
    "    w_pos = n_neg / (n_pos+n_neg)\n",
    "    w_neg = n_pos / (n_pos+n_neg)\n",
    "    \n",
    "    w_mask = (pos_mask*w_pos) + (neg_mask*w_neg)\n",
    "    \n",
    "    bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    return tf.reduce_mean(bce(y_true, y_pred)*w_mask)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=weighted_binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data pipelines\n",
      "\tDone...\n",
      "Using metric=val_loss and mode=min for EarlyStopping\n",
      "Epoch 1/10\n",
      "     69/Unknown - 7s 59ms/step - loss: 0.1220 - accuracy: 0.8496"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m es_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     24\u001b[0m     monitor\u001b[38;5;241m=\u001b[39mmonitor_metric, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, mode\u001b[38;5;241m=\u001b[39mmode, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t2\u001b[38;5;241m-\u001b[39mt1))\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Defining data pipelines\")\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y)\n",
    "test_ds = get_tf_pipeline(ts_x, ts_y)\n",
    "print('\\tDone...')\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','3_sentiment_analysis_custom_loss.log'))\n",
    "\n",
    "monitor_metric = 'val_loss'\n",
    "mode = 'min' if 'loss' in monitor_metric else 'max'\n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=10, callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
